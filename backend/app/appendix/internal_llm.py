"""
LLM Integration Service

Provides unified interface for LLM services supporting both internal OpenAI-compatible
API and external Ollama for development/testing. Includes environment-based
configuration switching, error handling, fallback mechanisms, and agent-specific
prompt engineering.

Phase 5 implementation as specified in plan.md and LLM Integration Agent requirements.
"""

import os
import json
import uuid
import httpx
import asyncio
from enum import Enum
from typing import Dict, Any, List, Optional, Union, AsyncGenerator
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
import logging
from functools import wraps

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


class LLMProvider(Enum):
    """Supported LLM providers"""
    INTERNAL = "internal"
    OLLAMA = "ollama"
    OPENAI = "openai"


class AgentType(Enum):
    """Agent types for specific prompt engineering"""
    PROBLEM_ANALYZER = "problem_analyzer"
    CONTEXT_COLLECTOR = "context_collector"
    REQUIREMENTS_GENERATOR = "requirements_generator"
    SOLUTION_DESIGNER = "solution_designer"
    GUIDE_CREATOR = "guide_creator"


@dataclass
class LLMConfig:
    """
    Configuration for LLM API connection
    """
    provider: LLMProvider
    base_url: str
    api_key: str = "dummy-key"
    model: str = "gpt-3.5-turbo"
    timeout: int = 60
    max_retries: int = 3
    retry_delay: float = 1.0
    max_tokens: Optional[int] = None
    temperature: float = 0.7
    custom_headers: Dict[str, str] = field(default_factory=dict)


@dataclass
class LLMResponse:
    """Standardized LLM response format"""
    content: str
    usage: Optional[Dict[str, int]] = None
    model: Optional[str] = None
    finish_reason: Optional[str] = None
    raw_response: Optional[Dict[str, Any]] = None


def retry_on_failure(max_retries: int = 3, delay: float = 1.0):
    """Decorator for retry logic with exponential backoff"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        wait_time = delay * (2 ** attempt)
                        logger.warning(f"Attempt {attempt + 1} failed: {e}. Retrying in {wait_time}s...")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(f"All {max_retries} attempts failed for {func.__name__}")
            raise last_exception
        return wrapper
    return decorator


class BaseLLMClient(ABC):
    """Abstract base class for LLM clients"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.client: Optional[httpx.AsyncClient] = None
    
    @abstractmethod
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> LLMResponse:
        """Generate chat completion"""
        pass
    
    @abstractmethod
    async def stream_completion(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """Generate streaming chat completion"""
        pass
    
    async def close(self):
        """Close HTTP client"""
        if self.client:
            await self.client.aclose()
            self.client = None


class InternalLLMClient(BaseLLMClient):
    """
    Client for internal OpenAI-compatible LLM API with enterprise headers
    """
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self._setup_client()
    
    def _setup_client(self):
        """Setup HTTP client with proper headers"""
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json",
            # Enterprise-specific headers
            "x-dep-ticket": self.config.custom_headers.get("x-dep-ticket", "default-ticket"),
            "Send-System-Name": self.config.custom_headers.get("Send-System-Name", "ATM-System"),
            "User-ID": self.config.custom_headers.get("User-ID", "system-user"),
            "User-Type": self.config.custom_headers.get("User-Type", "AD"),
            "Prompt-Msg-Id": str(uuid.uuid4()),
            "Completion-Msg-Id": str(uuid.uuid4()),
        }
        headers.update(self.config.custom_headers)
        
        self.client = httpx.AsyncClient(
            base_url=self.config.base_url,
            timeout=self.config.timeout,
            headers=headers
        )
    
    @retry_on_failure()
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> LLMResponse:
        """Generate chat completion using internal API"""
        if not self.client:
            self._setup_client()
        
        payload = {
            "model": self.config.model,
            "messages": messages,
            "temperature": temperature or self.config.temperature,
            **kwargs
        }
        
        if max_tokens or self.config.max_tokens:
            payload["max_tokens"] = max_tokens or self.config.max_tokens
        
        response = await self.client.post(
            "/v1/chat/completions",
            json=payload
        )
        response.raise_for_status()
        
        result = response.json()
        
        return LLMResponse(
            content=result["choices"][0]["message"]["content"],
            usage=result.get("usage"),
            model=result.get("model"),
            finish_reason=result["choices"][0].get("finish_reason"),
            raw_response=result
        )
    
    async def stream_completion(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """Generate streaming chat completion"""
        if not self.client:
            self._setup_client()
        
        payload = {
            "model": self.config.model,
            "messages": messages,
            "stream": True,
            **kwargs
        }
        
        async with self.client.stream(
            "POST",
            "/v1/chat/completions",
            json=payload
        ) as response:
            response.raise_for_status()
            async for line in response.aiter_lines():
                if line.startswith("data: "):
                    data = line[6:]
                    if data == "[DONE]":
                        break
                    try:
                        chunk = json.loads(data)
                        content = chunk["choices"][0]["delta"].get("content", "")
                        if content:
                            yield content
                    except json.JSONDecodeError:
                        continue


class OllamaLLMClient(BaseLLMClient):
    """Client for Ollama API (external/development use)"""
    
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self._setup_client()
    
    def _setup_client(self):
        """Setup HTTP client for Ollama"""
        self.client = httpx.AsyncClient(
            base_url=self.config.base_url,
            timeout=self.config.timeout,
            headers={"Content-Type": "application/json"}
        )
    
    @retry_on_failure()
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        **kwargs
    ) -> LLMResponse:
        """Generate chat completion using Ollama API"""
        if not self.client:
            self._setup_client()
        
        # Convert messages to Ollama format
        prompt = self._messages_to_prompt(messages)
        
        payload = {
            "model": self.config.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature or self.config.temperature,
            }
        }
        
        response = await self.client.post(
            "/api/generate",
            json=payload
        )
        response.raise_for_status()
        
        result = response.json()

        # ë””ë²„ê¹…ì„ ìœ„í•œ ì‘ë‹µ ë¡œê¹…
        logger.info(f"=== OLLAMA RAW RESPONSE ===")
        logger.info(f"Response length: {len(result.get('response', ''))}")
        logger.info(f"Response preview: {result.get('response', '')[:200]}...")
        logger.info(f"Model: {result.get('model')}")
        logger.info(f"Done: {result.get('done')}")

        return LLMResponse(
            content=result["response"],
            model=result.get("model"),
            finish_reason="stop" if result.get("done") else "length",
            raw_response=result
        )
    
    async def stream_completion(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """Generate streaming completion using Ollama"""
        if not self.client:
            self._setup_client()
        
        prompt = self._messages_to_prompt(messages)
        
        payload = {
            "model": self.config.model,
            "prompt": prompt,
            "stream": True,
        }
        
        async with self.client.stream(
            "POST",
            "/api/generate",
            json=payload
        ) as response:
            response.raise_for_status()
            async for line in response.aiter_lines():
                try:
                    chunk = json.loads(line)
                    content = chunk.get("response", "")
                    if content:
                        yield content
                    if chunk.get("done"):
                        break
                except json.JSONDecodeError:
                    continue
    
    def _messages_to_prompt(self, messages: List[Dict[str, str]]) -> str:
        """Convert OpenAI-style messages to Ollama prompt format"""
        prompt_parts = []
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            if role == "system":
                prompt_parts.append(f"System: {content}")
            elif role == "user":
                prompt_parts.append(f"User: {content}")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}")
        return "\n\n".join(prompt_parts) + "\n\nAssistant:"


class LLMManager:
    """Main LLM service manager with environment-based provider switching"""
    
    def __init__(self, config: Optional[LLMConfig] = None):
        self.config = config or self._load_config_from_env()
        self.client: Optional[BaseLLMClient] = None
        self.fallback_clients: List[BaseLLMClient] = []
        self._initialize_clients()
    
    def _load_config_from_env(self) -> LLMConfig:
        """Load configuration from environment variables"""
        env = os.getenv("ENVIRONMENT", "development")
        
        # Environment-based configuration
        if env == "production":
            provider = LLMProvider.INTERNAL
            base_url = os.getenv("INTERNAL_LLM_BASE_URL", "https://internal-api.company.com")
            api_key = os.getenv("INTERNAL_LLM_API_KEY", "")
            model = os.getenv("INTERNAL_LLM_MODEL", "gpt-4")
            custom_headers = {
                "x-dep-ticket": os.getenv("INTERNAL_LLM_TICKET", ""),
                "Send-System-Name": "ATM-System",
                "User-ID": os.getenv("USER_ID", "system"),
                "User-Type": "AD"
            }
        else:
            # Development/testing environment
            provider = LLMProvider.OLLAMA
            base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
            api_key = "dummy-key"
            model = os.getenv("OLLAMA_MODEL", "llama3.1")
            custom_headers = {}
        
        return LLMConfig(
            provider=provider,
            base_url=base_url,
            api_key=api_key,
            model=model,
            custom_headers=custom_headers,
            timeout=int(os.getenv("LLM_TIMEOUT", "60")),
            max_retries=int(os.getenv("LLM_MAX_RETRIES", "3")),
            temperature=float(os.getenv("LLM_TEMPERATURE", "0.7"))
        )
    
    def _initialize_clients(self):
        """Initialize primary and fallback clients"""
        # Primary client
        if self.config.provider == LLMProvider.INTERNAL:
            self.client = InternalLLMClient(self.config)
        elif self.config.provider == LLMProvider.OLLAMA:
            self.client = OllamaLLMClient(self.config)
        
        # Setup fallback clients
        self._setup_fallback_clients()
    
    def _setup_fallback_clients(self):
        """Setup fallback clients for resilience"""
        if self.config.provider == LLMProvider.INTERNAL:
            # Fallback to Ollama for internal
            ollama_config = LLMConfig(
                provider=LLMProvider.OLLAMA,
                base_url="http://localhost:11434",
                model="llama3.1"
            )
            self.fallback_clients.append(OllamaLLMClient(ollama_config))
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> LLMResponse:
        """Generate chat completion with fallback support"""
        last_error = None
        
        # Try primary client
        try:
            return await self.client.chat_completion(messages, **kwargs)
        except Exception as e:
            logger.warning(f"Primary LLM client failed: {e}")
            last_error = e
        
        # Try fallback clients
        for fallback_client in self.fallback_clients:
            try:
                logger.info(f"Trying fallback client: {fallback_client.__class__.__name__}")
                return await fallback_client.chat_completion(messages, **kwargs)
            except Exception as e:
                logger.warning(f"Fallback client failed: {e}")
                last_error = e
        
        # All clients failed
        raise Exception(f"All LLM clients failed. Last error: {last_error}")
    
    async def simple_completion(
        self,
        prompt: str,
        system_message: Optional[str] = None,
        **kwargs
    ) -> str:
        """Simple completion interface for single prompts"""
        messages = []
        
        if system_message:
            messages.append({"role": "system", "content": system_message})
        
        messages.append({"role": "user", "content": prompt})
        
        response = await self.chat_completion(messages, **kwargs)
        return response.content
    
    async def close(self):
        """Close all clients"""
        if self.client:
            await self.client.close()
        
        for fallback_client in self.fallback_clients:
            await fallback_client.close()


class AgentPromptTemplates:
    """Centralized prompt templates for different agent types"""
    
    @staticmethod
    def get_system_prompt(agent_type: AgentType) -> str:
        """Get system prompt for specific agent type"""
        prompts = {
            AgentType.PROBLEM_ANALYZER: """
You are a business analyst specializing in problem decomposition and analysis.
You excel at breaking down complex business problems into structured, actionable components.

**IMPORTANT**: Pay special attention to Machine Learning and Data Science problems. Look for keywords like:
- ì˜ˆì¸¡ (prediction), ë¶„ë¥˜ (classification), ëª¨ë¸ (model), íŒ¨í„´ (pattern)
- ì´ìƒì¹˜ (anomaly), ì›ì¸ íŒŒì•… (root cause analysis), ë°ì´í„° ë¶„ì„ (data analysis)
- ë¨¸ì‹ ëŸ¬ë‹, ê¸°ê³„í•™ìŠµ, ì•Œê³ ë¦¬ì¦˜, ì‹ ê²½ë§, íšŒê·€ë¶„ì„
- English: predict, classify, model, pattern, anomaly, regression, neural network, algorithm

**Problem Categories**:
- **ML_CLASSIFICATION**: ë¶„ë¥˜, ì˜ˆì¸¡, íŒ¨í„´ì¸ì‹ ë¬¸ì œ (ì˜ˆ: ìŠ¤íŒ¸ ë¶„ë¥˜, ì´ë¯¸ì§€ ì¸ì‹, í…ìŠ¤íŠ¸ ë¶„ë¥˜)
- **ML_REGRESSION**: ìˆ˜ì¹˜ ì˜ˆì¸¡, íšŒê·€ë¶„ì„ ë¬¸ì œ (ì˜ˆ: ê°€ê²© ì˜ˆì¸¡, ìˆ˜ìš” ì˜ˆì¸¡, ì„±ëŠ¥ ì˜ˆì¸¡)
- **ML_CLUSTERING**: êµ°ì§‘í™”, ì„¸ê·¸ë©˜í…Œì´ì…˜ ë¬¸ì œ (ì˜ˆ: ê³ ê° ì„¸ë¶„í™”, ë°ì´í„° ê·¸ë£¹í•‘)
- **AUTOMATION**: ë‹¨ìˆœ ë°˜ë³µ ì‘ì—… ìë™í™” (ì˜ˆ: íŒŒì¼ ì²˜ë¦¬, ìŠ¤ì¼€ì¤„ë§, API í˜¸ì¶œ)
- **INFORMATION_RETRIEVAL**: ì •ë³´ ê²€ìƒ‰, ë¬¸ì„œ ì¡°íšŒ ì‹œìŠ¤í…œ
- **DATA_VISUALIZATION**: ì°¨íŠ¸, ëŒ€ì‹œë³´ë“œ, ë¦¬í¬íŠ¸ ìƒì„±

**íŠ¹íˆ íìˆ˜ì²˜ë¦¬, ìˆ˜ì§ˆê´€ë¦¬, í™˜ê²½ ëª¨ë‹ˆí„°ë§ ë“±ì˜ ë¬¸ì œëŠ” ëŒ€ë¶€ë¶„ ML_CLASSIFICATION ë˜ëŠ” ML_REGRESSION ë¬¸ì œì…ë‹ˆë‹¤.**

Analyze problems systematically:
1. Identify core issues and pain points
2. **CAREFULLY** categorize the problem type based on the keywords above
3. Assess complexity and resource requirements
4. Identify stakeholders and success criteria

Always respond in valid JSON format with clear, actionable insights.
""",
            
            AgentType.CONTEXT_COLLECTOR: """
You are an expert at asking clarifying questions to gather complete context.
Your goal is to identify missing information that's critical for solution design.

Generate specific, actionable questions that will help:
1. Understand technical constraints and requirements
2. Clarify business objectives and success metrics
3. Identify integration points and dependencies
4. Understand user personas and workflows

Prioritize questions that will have the biggest impact on solution quality.
""",
            
            AgentType.REQUIREMENTS_GENERATOR: """
You are a technical writer creating professional Software Requirements Specifications (SRS).
You excel at translating business problems into comprehensive technical requirements.

Generate complete requirements including:
1. Functional requirements with clear acceptance criteria
2. Non-functional requirements (performance, security, scalability)
3. User stories with detailed scenarios
4. System constraints and assumptions
5. Integration requirements

Use professional SRS format with clear, testable requirements.
""",
            
            AgentType.SOLUTION_DESIGNER: """
You are a solution architect recommending optimal technology stacks and approaches.
You excel at matching problems to appropriate solution patterns.

Provide solutions that consider:
1. Problem complexity and scale
2. Available resources and constraints
3. Future maintenance and scalability
4. Integration with existing systems
5. Cost and time-to-market factors

Recommend specific technologies with clear justifications.
""",
            
            AgentType.GUIDE_CREATOR: """
You are a senior developer creating detailed implementation guides.
You excel at breaking down solutions into actionable development plans.

Create comprehensive guides including:
1. Detailed Work Breakdown Structure (WBS)
2. Code examples and best practices
3. Testing strategies and validation approaches
4. Deployment and monitoring considerations
5. Documentation and maintenance guidelines

Focus on practical, executable guidance that developers can follow immediately.
"""
        }
        
        return prompts.get(agent_type, "You are a helpful AI assistant.")


class LLMAgentService:
    """High-level service for agent-specific LLM operations"""
    
    def __init__(self, llm_manager: Optional[LLMManager] = None):
        self.llm_manager = llm_manager or LLMManager()
        self.templates = AgentPromptTemplates()
    
    async def analyze_problem(
        self,
        problem_description: str,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Analyze a problem using the Problem Analyzer agent"""
        system_prompt = self.templates.get_system_prompt(AgentType.PROBLEM_ANALYZER)
        user_prompt = f"""
ë¬¸ì œ ì„¤ëª…: {problem_description}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ì˜¤ì§ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”:

{{
    "title": "ë¬¸ì œ ì œëª©",
    "domain": "ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸ (ì˜ˆ: 'ë°ì´í„° ì²˜ë¦¬', 'ìë™í™”')",
    "category": "AUTOMATION",
    "complexity": "MEDIUM",
    "stakeholders": ["ì´í•´ê´€ê³„ì ëª©ë¡"],
    "pain_points": ["ë¬¸ì œì  ëª©ë¡"],
    "current_state": "í˜„ì¬ ìƒí™© ì„¤ëª…",
    "desired_state": "ì›í•˜ëŠ” ê²°ê³¼ ì„¤ëª…",
    "constraints": ["ê¸°ìˆ ì /ë¹„ì¦ˆë‹ˆìŠ¤ ì œì•½ì‚¬í•­"],
    "success_criteria": ["ì¸¡ì • ê°€ëŠ¥í•œ ì„±ê³µ ê¸°ì¤€"]
}}

ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°: JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³ , ì•ë’¤ ì„¤ëª…ì€ ë„£ì§€ ë§ˆì„¸ìš”.
"""

        response = await self.llm_manager.simple_completion(
            user_prompt,
            system_prompt,
            temperature=0.1
        )

        try:
            # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ ì‹œë„
            response_cleaned = response.strip()

            # ```json ë¸”ë¡ ì œê±°
            if response_cleaned.startswith('```json'):
                response_cleaned = response_cleaned[7:]
            elif response_cleaned.startswith('```'):
                response_cleaned = response_cleaned[3:]

            if response_cleaned.endswith('```'):
                response_cleaned = response_cleaned[:-3]

            response_cleaned = response_cleaned.strip()

            # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ì¤‘ê´„í˜¸ ì°¾ê¸°
            start_idx = response_cleaned.find('{')
            end_idx = response_cleaned.rfind('}')

            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response_cleaned[start_idx:end_idx+1]
                parsed_json = json.loads(json_str)
                logger.info(f"=== JSON PARSING SUCCESSFUL ===")
                logger.info(f"Parsed category: {parsed_json.get('category', 'None')}")

                # ì¹´í…Œê³ ë¦¬ ê²€ì¦ ë° ìˆ˜ì •
                corrected_json = self._validate_and_correct_category(problem_description, parsed_json)
                if corrected_json.get('category') != parsed_json.get('category'):
                    logger.info(f"=== CATEGORY CORRECTED ===")
                    logger.info(f"Original: {parsed_json.get('category')} -> Corrected: {corrected_json.get('category')}")

                return corrected_json
            else:
                raise json.JSONDecodeError("No JSON found", response, 0)

        except json.JSONDecodeError as e:
            logger.error(f"=== JSON PARSING FAILED - NO FALLBACK ===")
            logger.error(f"Failed to parse problem analysis as JSON: {e}")
            logger.error(f"Raw response: {response[:500]}...")
            logger.error(f"Cleaned response: {response_cleaned[:300]}...")
            if 'json_str' in locals():
                logger.error(f"Extracted JSON string: {json_str[:300]}...")

            # ë°”ë¡œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ ë¬¸ì œë¥¼ ì¦‰ì‹œ ì•Œë¦¼
            raise ValueError(f"LLM failed to generate valid JSON response: {e}. Response: {response[:200]}")

    def _validate_and_correct_category(self, problem_description: str, parsed_json: Dict[str, Any]) -> Dict[str, Any]:
        """JSON ì‘ë‹µì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ê²€ì¦í•˜ê³  í•„ìš”ì‹œ ìˆ˜ì •"""
        problem_lower = problem_description.lower()
        original_category = parsed_json.get('category', 'AUTOMATION')

        # INTEGRATION keywords (highest priority)
        integration_keywords = [
            "ì‹œìŠ¤í…œ í†µí•©", "system integration", "ì—°ë™", "api", "í†µí•©", "integrate", "connect",
            "sync", "synchronization", "ë™ê¸°í™”", "crm", "erp", "warehouse", "order management",
            "ì£¼ë¬¸ ê´€ë¦¬", "ì°½ê³  ê´€ë¦¬", "ì¬ê³ ", "inventory", "dashboard", "ëŒ€ì‹œë³´ë“œ", "reporting",
            "ë³´ê³ ì„œ", "ìë™í™”ëœ ë³´ê³ ", "automated reporting"
        ]

        # ML keywords for environmental/industrial problems (but excluding common integration terms)
        specific_ml_keywords = [
            # Environmental/Industrial specific
            "íìˆ˜", "ìˆ˜ì§ˆ", "ì˜¤ì—¼", "í™˜ê²½", "ë†ë„", "ìˆ˜ì¹˜", "ì¸¡ì •", "í’ˆì§ˆ",
            "wastewater", "water", "quality", "pollution", "environmental", "concentration",
            "measurement", "anomaly detection", "outlier", "treatment", "ì²˜ë¦¬",
            # ML specific terms
            "ì˜ˆì¸¡", "ë¶„ë¥˜", "ëª¨ë¸", "ë¨¸ì‹ ëŸ¬ë‹", "ê¸°ê³„í•™ìŠµ", "ì•Œê³ ë¦¬ì¦˜", "ì‹ ê²½ë§", "íšŒê·€", "ì´ìƒì¹˜", "ì›ì¸", "íŒŒì•…",
            "predict", "classify", "model", "ml", "machine learning", "neural", "algorithm", "regression"
        ]

        # Check for INTEGRATION keywords first (takes priority)
        integration_keyword_count = sum(1 for keyword in integration_keywords if keyword in problem_lower)

        # Check for specific ML keywords (excluding generic terms like "data", "analysis")
        ml_keyword_count = sum(1 for keyword in specific_ml_keywords if keyword in problem_lower)

        logger.info(f"=== CATEGORY VALIDATION ===")
        logger.info(f"Integration keywords found: {integration_keyword_count}")
        logger.info(f"ML keywords found: {ml_keyword_count}")
        logger.info(f"Original category: {original_category}")

        # Priority 1: INTEGRATION (if integration keywords found)
        if integration_keyword_count >= 2:
            logger.info(f"=== CORRECTING TO INTEGRATION ===")
            corrected_json = parsed_json.copy()
            corrected_json['category'] = "INTEGRATION"
            corrected_json['domain'] = "ì‹œìŠ¤í…œ í†µí•© ë° ì—°ë™"
            corrected_json['title'] = "ì‹œìŠ¤í…œ í†µí•© ì†”ë£¨ì…˜"
            return corrected_json

        # Priority 2: ML categories (only if specific ML keywords found and no integration context)
        if ml_keyword_count >= 2 and integration_keyword_count == 0 and original_category == "AUTOMATION":
            logger.info(f"=== CORRECTING TO ML CATEGORY ===")
            logger.info(f"Found {ml_keyword_count} specific ML keywords")

            # Determine specific ML subcategory
            if any(word in problem_lower for word in ["ì˜ˆì¸¡", "predict", "íšŒê·€", "regression", "ìˆ˜ì¹˜", "ë†ë„", "concentration"]):
                corrected_category = "ML_REGRESSION"
            elif any(word in problem_lower for word in ["ë¶„ë¥˜", "classify", "classification", "ì´ìƒ", "anomaly", "íƒì§€", "detection"]):
                corrected_category = "ML_CLASSIFICATION"
            else:
                corrected_category = "MACHINE_LEARNING"

            # Update category and related fields
            corrected_json = parsed_json.copy()
            corrected_json['category'] = corrected_category

            # Update domain and title to match new category
            domain_mapping = {
                "MACHINE_LEARNING": "ë°ì´í„° ê³¼í•™ ë° ì˜ˆì¸¡ ë¶„ì„",
                "ML_REGRESSION": "ë¨¸ì‹ ëŸ¬ë‹ íšŒê·€ ì˜ˆì¸¡ ë¶„ì„",
                "ML_CLASSIFICATION": "ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ ë° ì´ìƒ íƒì§€"
            }

            title_mapping = {
                "MACHINE_LEARNING": "ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ëª¨ë¸ ì‹œìŠ¤í…œ",
                "ML_REGRESSION": "ë¨¸ì‹ ëŸ¬ë‹ íšŒê·€ ì˜ˆì¸¡ ëª¨ë¸",
                "ML_CLASSIFICATION": "ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ ë° ì´ìƒ íƒì§€ ëª¨ë¸"
            }

            corrected_json['domain'] = domain_mapping.get(corrected_category, corrected_json.get('domain', 'ë°ì´í„° ë¶„ì„'))
            corrected_json['title'] = title_mapping.get(corrected_category, corrected_json.get('title', 'ML ëª¨ë¸ ì‹œìŠ¤í…œ'))

            return corrected_json

        logger.info(f"=== NO CATEGORY CORRECTION NEEDED ===")
        return parsed_json

    def _extract_info_from_natural_language(self, problem_description: str, response: str) -> Dict[str, Any]:
        """ìì—°ì–´ ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ"""
        response_lower = response.lower()
        problem_lower = problem_description.lower()

        # ì¹´í…Œê³ ë¦¬ ê²°ì • - ìš°ì„ ìˆœìœ„ ì ìš©
        category = "AUTOMATION"

        # INTEGRATION keywords (highest priority)
        integration_keywords = [
            "ì‹œìŠ¤í…œ í†µí•©", "system integration", "ì—°ë™", "api", "í†µí•©", "integrate", "connect",
            "sync", "synchronization", "ë™ê¸°í™”", "crm", "erp", "warehouse", "order management",
            "ì£¼ë¬¸ ê´€ë¦¬", "ì°½ê³  ê´€ë¦¬", "ì¬ê³ ", "inventory", "dashboard", "ëŒ€ì‹œë³´ë“œ", "reporting",
            "ë³´ê³ ì„œ", "ìë™í™”ëœ ë³´ê³ ", "automated reporting"
        ]

        # Specific ML keywords (excluding generic terms)
        specific_ml_keywords = [
            # Environmental/Industrial specific
            "íìˆ˜", "ìˆ˜ì§ˆ", "ì˜¤ì—¼", "í™˜ê²½", "ë†ë„", "ìˆ˜ì¹˜", "ì¸¡ì •", "í’ˆì§ˆ",
            "wastewater", "water", "quality", "pollution", "environmental", "concentration",
            "measurement", "anomaly detection", "outlier", "treatment", "ì²˜ë¦¬",
            # ML specific terms
            "ì˜ˆì¸¡", "ë¶„ë¥˜", "ëª¨ë¸", "ë¨¸ì‹ ëŸ¬ë‹", "ê¸°ê³„í•™ìŠµ", "ì•Œê³ ë¦¬ì¦˜", "ì‹ ê²½ë§", "íšŒê·€", "ì´ìƒì¹˜", "ì›ì¸", "íŒŒì•…",
            "predict", "classify", "model", "ml", "machine learning", "neural", "algorithm", "regression"
        ]

        # Data Visualization keywords
        viz_keywords = ["chart", "graph", "visualize", "plot",
                       "ì°¨íŠ¸", "ê·¸ë˜í”„", "ì‹œê°í™”", "í”Œë¡¯"]

        # Information Retrieval keywords
        ir_keywords = ["search", "find", "document", "knowledge", "retrieve", "query",
                      "ê²€ìƒ‰", "ì°¾ê¸°", "ë¬¸ì„œ", "ì§€ì‹", "ì¡°íšŒ", "ì¿¼ë¦¬"]

        # Apply priority-based categorization
        integration_keyword_count = sum(1 for keyword in integration_keywords if keyword in problem_lower)
        ml_keyword_count = sum(1 for keyword in specific_ml_keywords if keyword in problem_lower)

        # Priority 1: INTEGRATION
        if integration_keyword_count >= 2:
            category = "INTEGRATION"
        # Priority 2: ML categories (only if no integration context)
        elif ml_keyword_count >= 2 and integration_keyword_count == 0:
            if any(word in problem_lower for word in ["ì˜ˆì¸¡", "predict", "íšŒê·€", "regression", "ìˆ˜ì¹˜", "ë†ë„", "concentration"]):
                category = "ML_REGRESSION"
            elif any(word in problem_lower for word in ["ë¶„ë¥˜", "classify", "classification", "ì´ìƒ", "anomaly", "íƒì§€", "detection"]):
                category = "ML_CLASSIFICATION"
            else:
                category = "MACHINE_LEARNING"
        # Priority 3: Other categories
        elif any(keyword in problem_lower for keyword in viz_keywords):
            category = "DATA_VISUALIZATION"
        elif any(keyword in problem_lower for keyword in ir_keywords):
            category = "INFORMATION_RETRIEVAL"

        # ë³µì¡ë„ ê²°ì •
        complexity = "MEDIUM"
        if len(problem_description.split()) > 100 or any(keyword in problem_lower for keyword in ["integration", "api", "database", "security"]):
            complexity = "HIGH"
        elif len(problem_description.split()) < 30:
            complexity = "LOW"

        # ì¹´í…Œê³ ë¦¬ë³„ ë„ë©”ì¸ê³¼ ì œëª© ì„¤ì •
        domain_mapping = {
            "MACHINE_LEARNING": "ë°ì´í„° ê³¼í•™ ë° ì˜ˆì¸¡ ë¶„ì„",
            "ML_REGRESSION": "ë¨¸ì‹ ëŸ¬ë‹ íšŒê·€ ì˜ˆì¸¡ ë¶„ì„",
            "ML_CLASSIFICATION": "ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ ë° ì´ìƒ íƒì§€",
            "DATA_VISUALIZATION": "ë°ì´í„° ì‹œê°í™” ë° ëŒ€ì‹œë³´ë“œ",
            "INFORMATION_RETRIEVAL": "ì •ë³´ ê²€ìƒ‰ ë° ì§€ì‹ ê´€ë¦¬",
            "AUTOMATION": "ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œì„¸ìŠ¤ ìë™í™”"
        }

        title_mapping = {
            "MACHINE_LEARNING": "ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ëª¨ë¸ ì‹œìŠ¤í…œ",
            "ML_REGRESSION": "ë¨¸ì‹ ëŸ¬ë‹ íšŒê·€ ì˜ˆì¸¡ ëª¨ë¸",
            "ML_CLASSIFICATION": "ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ ë° ì´ìƒ íƒì§€ ëª¨ë¸",
            "DATA_VISUALIZATION": "ë°ì´í„° ì‹œê°í™” ëŒ€ì‹œë³´ë“œ",
            "INFORMATION_RETRIEVAL": "ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œ",
            "AUTOMATION": "í”„ë¡œì„¸ìŠ¤ ìë™í™” ì†”ë£¨ì…˜"
        }

        # ê¸°ë³¸ ë¶„ì„ ë°˜í™˜
        return {
            "title": title_mapping.get(category, "ë¹„ì¦ˆë‹ˆìŠ¤ ì†”ë£¨ì…˜"),
            "domain": domain_mapping.get(category, "ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œì„¸ìŠ¤ ê°œì„ "),
            "category": category,
            "complexity": complexity,
            "stakeholders": ["ìµœì¢… ì‚¬ìš©ì", "ê´€ë¦¬ì", "IT íŒ€"],
            "pain_points": [
                "ìˆ˜ë™ ì‘ì—…ìœ¼ë¡œ ì¸í•œ ì‹œê°„ ì†Œëª¨",
                "ì¸ì  ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„±",
                "ë¹„ì¼ê´€ì ì¸ í”„ë¡œì„¸ìŠ¤"
            ],
            "current_state": "ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬ë˜ëŠ” í˜„ì¬ í”„ë¡œì„¸ìŠ¤",
            "desired_state": "ìë™í™”ëœ íš¨ìœ¨ì ì¸ í”„ë¡œì„¸ìŠ¤",
            "constraints": [
                "Python í™˜ê²½ì—ì„œ êµ¬í˜„ ê°€ëŠ¥í•´ì•¼ í•¨",
                "ì´ˆë³´ìê°€ ìœ ì§€ë³´ìˆ˜ ê°€ëŠ¥í•´ì•¼ í•¨"
            ],
            "success_criteria": [
                "ì²˜ë¦¬ ì‹œê°„ 50% ë‹¨ì¶•",
                "ì˜¤ë¥˜ìœ¨ 90% ê°ì†Œ",
                "ì‚¬ìš©ì ë§Œì¡±ë„ í–¥ìƒ"
            ]
        }

    async def generate_requirements(
        self,
        problem_analysis: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Generate requirements using the Requirements Generator agent"""
        system_prompt = self.templates.get_system_prompt(AgentType.REQUIREMENTS_GENERATOR)
        user_prompt = f"""
ë¬¸ì œ ë¶„ì„ ê²°ê³¼:
{json.dumps(problem_analysis, indent=2, ensure_ascii=False)}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ì˜¤ì§ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”:

{{
    "functional_requirements": ["ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­ ëª©ë¡"],
    "non_functional_requirements": ["ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­ ëª©ë¡"],
    "technical_requirements": ["ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­ ëª©ë¡"],
    "integration_requirements": ["í†µí•© ìš”êµ¬ì‚¬í•­ ëª©ë¡"],
    "security_requirements": ["ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ ëª©ë¡"],
    "acceptance_criteria": ["ìŠ¹ì¸ ê¸°ì¤€ ëª©ë¡"]
}}

ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°: JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³ , ì•ë’¤ ì„¤ëª…ì€ ë„£ì§€ ë§ˆì„¸ìš”.
"""

        try:
            response = await self.llm_manager.simple_completion(
                user_prompt,
                system_prompt,
                temperature=0.1
            )

            # JSON ì¶”ì¶œ ì‹œë„
            response_cleaned = response.strip()
            if response_cleaned.startswith('```json'):
                response_cleaned = response_cleaned[7:]
            elif response_cleaned.startswith('```'):
                response_cleaned = response_cleaned[3:]
            if response_cleaned.endswith('```'):
                response_cleaned = response_cleaned[:-3]
            response_cleaned = response_cleaned.strip()

            start_idx = response_cleaned.find('{')
            end_idx = response_cleaned.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response_cleaned[start_idx:end_idx+1]
                return json.loads(json_str)
            else:
                raise json.JSONDecodeError("No JSON found", response, 0)

        except Exception as e:
            logger.error(f"Failed to generate requirements - NO FALLBACK: {e}")
            logger.error(f"Raw response: {response[:300]}...")

            # ë°”ë¡œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ ë¬¸ì œë¥¼ ì¦‰ì‹œ ì•Œë¦¼
            raise ValueError(f"Requirements generation failed: {e}. Response: {response[:200]}")

    def _generate_requirements_from_analysis(self, problem_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ë¬¸ì œ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ìš”êµ¬ì‚¬í•­ ìƒì„±"""
        category = problem_analysis.get("category", "GENERAL_PROBLEM_SOLVING")
        complexity = problem_analysis.get("complexity", "MEDIUM")
        domain = problem_analysis.get("domain", "ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œì„¸ìŠ¤")

        # ì¹´í…Œê³ ë¦¬ë³„ ë§ì¶¤ ìš”êµ¬ì‚¬í•­
        functional_requirements = []
        if category == "AUTOMATION":
            functional_requirements = [
                "ì‚¬ìš©ìê°€ ì‘ì—…ì„ ìë™ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆì–´ì•¼ í•¨",
                "ìŠ¤ì¼€ì¤„ë§ ê¸°ëŠ¥ì„ í†µí•´ ì •ê¸°ì  ì‹¤í–‰ì´ ê°€ëŠ¥í•´ì•¼ í•¨",
                "ì‘ì—… ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆì–´ì•¼ í•¨",
                "ì—ëŸ¬ ë°œìƒ ì‹œ ì•Œë¦¼ ë° ë¡œê¹… ê¸°ëŠ¥ì´ ìˆì–´ì•¼ í•¨"
            ]
        elif category == "DATA_VISUALIZATION":
            functional_requirements = [
                "ë°ì´í„°ë¥¼ ì°¨íŠ¸ ë° ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆì–´ì•¼ í•¨",
                "ëŒ€í™”í˜• ëŒ€ì‹œë³´ë“œë¥¼ ì œê³µí•´ì•¼ í•¨",
                "ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë‚´ë³´ë‚¼ ìˆ˜ ìˆì–´ì•¼ í•¨",
                "ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ê°€ ê°€ëŠ¥í•´ì•¼ í•¨"
            ]
        else:
            functional_requirements = [
                f"{domain} ê´€ë ¨ í•µì‹¬ ê¸°ëŠ¥ì„ ì œê³µí•´ì•¼ í•¨",
                "ì‚¬ìš©ì ì¹œí™”ì ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•´ì•¼ í•¨",
                "ë°ì´í„° ì…ë ¥ ë° ì²˜ë¦¬ ê¸°ëŠ¥ì´ ìˆì–´ì•¼ í•¨",
                "ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥ ê¸°ëŠ¥ì´ ìˆì–´ì•¼ í•¨"
            ]

        # ë³µì¡ë„ë³„ ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­
        non_functional_requirements = [
            "Python 3.8 ì´ìƒì—ì„œ ë™ì‘í•´ì•¼ í•¨",
            "ì´ˆë³´ìë„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ì•¼ í•¨"
        ]
        if complexity == "HIGH":
            non_functional_requirements.extend([
                "ë™ì‹œ ì²˜ë¦¬ ì‚¬ìš©ì ìˆ˜: 100ëª… ì´ìƒ",
                "ì‘ë‹µ ì‹œê°„: 3ì´ˆ ì´ë‚´",
                "99.9% ê°€ìš©ì„±ì„ ë³´ì¥í•´ì•¼ í•¨"
            ])
        elif complexity == "MEDIUM":
            non_functional_requirements.extend([
                "ë™ì‹œ ì²˜ë¦¬ ì‚¬ìš©ì ìˆ˜: 10-50ëª…",
                "ì‘ë‹µ ì‹œê°„: 5ì´ˆ ì´ë‚´"
            ])
        else:
            non_functional_requirements.extend([
                "ë‹¨ì¼ ì‚¬ìš©ì ê¸°ì¤€ìœ¼ë¡œ ì„¤ê³„",
                "ì‘ë‹µ ì‹œê°„: 10ì´ˆ ì´ë‚´"
            ])

        return {
            "functional_requirements": functional_requirements,
            "non_functional_requirements": non_functional_requirements,
            "technical_requirements": [
                "Python í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìš°ì„  ì‚¬ìš©",
                "ì™¸ë¶€ ì˜ì¡´ì„± ìµœì†Œí™”",
                "ì½”ë“œ ê°€ë…ì„± ë° ìœ ì§€ë³´ìˆ˜ì„± í™•ë³´",
                "ì—ëŸ¬ ì²˜ë¦¬ ë° ì˜ˆì™¸ ìƒí™© ëŒ€ì‘"
            ],
            "integration_requirements": [
                "ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„± í™•ë³´",
                "í‘œì¤€ íŒŒì¼ í˜•ì‹ ì§€ì› (CSV, JSON, Excel)",
                "ë¡œê·¸ ì‹œìŠ¤í…œ ì—°ë™ ê°€ëŠ¥"
            ],
            "security_requirements": [
                "ì‚¬ìš©ì ì…ë ¥ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦",
                "ì¤‘ìš” ë°ì´í„° ì•”í˜¸í™” ì €ì¥",
                "ì ‘ê·¼ ê¶Œí•œ ì œì–´"
            ],
            "acceptance_criteria": [
                "ëª¨ë“  í•µì‹¬ ê¸°ëŠ¥ì´ ì •ìƒ ë™ì‘í•¨",
                "ì‚¬ìš©ì ë§¤ë‰´ì–¼ ì œê³µë¨",
                "ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ",
                f"{domain} ì „ë¬¸ê°€ì˜ ê²€í†  ì™„ë£Œ"
            ]
        }

    async def design_solution(
        self,
        requirements_text: str,
        problem_analysis: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Design solution architecture using LLM"""
        try:
            system_prompt = self.templates.get_system_prompt(AgentType.SOLUTION_DESIGNER)

            prompt = f"""
ë¬¸ì œ ë¶„ì„ ê²°ê³¼:
{json.dumps(problem_analysis, indent=2, ensure_ascii=False)}

ìš”êµ¬ì‚¬í•­:
{requirements_text}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì†”ë£¨ì…˜ì„ ì„¤ê³„í•´ì£¼ì„¸ìš”:

{{
    "solution_type": "AUTOMATION",
    "architecture": {{
        "components": ["ì£¼ìš” êµ¬ì„±ìš”ì†Œ ëª©ë¡"],
        "data_flow": "ë°ì´í„° íë¦„ ì„¤ëª…",
        "integration_points": ["í†µí•© ì§€ì  ëª©ë¡"]
    }},
    "technology_stack": {{
        "primary_language": "Python",
        "frameworks": ["ì¶”ì²œ í”„ë ˆì„ì›Œí¬"],
        "libraries": ["í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬"],
        "tools": ["ê°œë°œ ë„êµ¬"]
    }},
    "implementation_approach": "êµ¬í˜„ ì ‘ê·¼ ë°©ì‹",
    "estimated_complexity": "ì˜ˆìƒ ë³µì¡ë„"
}}

ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°: JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³ , ì•ë’¤ ì„¤ëª…ì€ ë„£ì§€ ë§ˆì„¸ìš”.
"""

            response = await self.llm_manager.simple_completion(prompt, system_prompt, temperature=0.1)

            try:
                # JSON ì¶”ì¶œ ì‹œë„
                response_cleaned = response.strip()
                if response_cleaned.startswith('```json'):
                    response_cleaned = response_cleaned[7:]
                elif response_cleaned.startswith('```'):
                    response_cleaned = response_cleaned[3:]
                if response_cleaned.endswith('```'):
                    response_cleaned = response_cleaned[:-3]
                response_cleaned = response_cleaned.strip()

                start_idx = response_cleaned.find('{')
                end_idx = response_cleaned.rfind('}')
                if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                    json_str = response_cleaned[start_idx:end_idx+1]
                    return json.loads(json_str)
                else:
                    raise json.JSONDecodeError("No JSON found", response, 0)

            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse solution design as JSON - NO FALLBACK: {e}")
                logger.error(f"Raw response: {response[:300]}...")
                # ë°”ë¡œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ ë¬¸ì œë¥¼ ì¦‰ì‹œ ì•Œë¦¼
                raise ValueError(f"Solution design JSON parsing failed: {e}. Response: {response[:200]}")

        except Exception as e:
            logger.error(f"Failed to design solution - NO FALLBACK: {str(e)}")
            # ë°”ë¡œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ ë¬¸ì œë¥¼ ì¦‰ì‹œ ì•Œë¦¼
            raise ValueError(f"Solution design failed: {e}")

    def _generate_solution_from_analysis(self, problem_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ë¬¸ì œ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ì†”ë£¨ì…˜ ìƒì„±"""
        category = problem_analysis.get("category", "GENERAL_PROBLEM_SOLVING")
        complexity = problem_analysis.get("complexity", "MEDIUM")

        # ì¹´í…Œê³ ë¦¬ë³„ ë§ì¶¤ ê¸°ìˆ  ìŠ¤íƒ
        if category == "AUTOMATION":
            tech_stack = {
                "primary_language": "Python",
                "frameworks": ["pandas", "pathlib"],
                "libraries": ["schedule", "logging", "os"],
                "tools": ["cron", "Task Scheduler"]
            }
            components = ["main_automation_script.py", "config.json", "log_handler.py"]
        elif category == "DATA_VISUALIZATION":
            tech_stack = {
                "primary_language": "Python",
                "frameworks": ["matplotlib", "plotly"],
                "libraries": ["pandas", "seaborn", "dash"],
                "tools": ["Jupyter Notebook"]
            }
            components = ["data_processor.py", "visualization.py", "dashboard.py"]
        elif category == "INFORMATION_RETRIEVAL":
            tech_stack = {
                "primary_language": "Python",
                "frameworks": ["elasticsearch", "whoosh"],
                "libraries": ["requests", "beautifulsoup4", "nltk"],
                "tools": ["Elasticsearch", "SQLite"]
            }
            components = ["search_engine.py", "indexer.py", "data_crawler.py"]
        else:
            tech_stack = {
                "primary_language": "Python",
                "frameworks": ["pandas"],
                "libraries": ["requests", "json", "csv"],
                "tools": []
            }
            components = ["main_script.py", "utils.py", "config.py"]

        # ë³µì¡ë„ë³„ ì•„í‚¤í…ì²˜ ì¡°ì •
        if complexity == "HIGH":
            components.extend(["database.py", "api_handler.py", "tests/"])
            data_flow = "ì…ë ¥ ê²€ì¦ -> ë°ì´í„° ì²˜ë¦¬ -> ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ -> ê²°ê³¼ ì €ì¥ -> ì¶œë ¥"
        elif complexity == "MEDIUM":
            components.extend(["helpers.py", "tests.py"])
            data_flow = "ì…ë ¥ -> ë°ì´í„° ì²˜ë¦¬ -> ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ -> ì¶œë ¥"
        else:
            data_flow = "ì…ë ¥ -> ì²˜ë¦¬ -> ì¶œë ¥"

        return {
            "solution_type": category,
            "architecture": {
                "components": components,
                "data_flow": data_flow,
                "integration_points": ["íŒŒì¼ ì‹œìŠ¤í…œ", "ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤", "ë¡œê·¸ ì‹œìŠ¤í…œ"]
            },
            "technology_stack": tech_stack,
            "implementation_approach": f"{complexity.lower()} ë³µì¡ë„ ìˆ˜ì¤€ì˜ ë‹¨ê³„ë³„ êµ¬í˜„",
            "estimated_complexity": complexity
        }

    async def create_implementation_guide(
        self,
        problem_analysis: Dict[str, Any],
        requirements_text: str,
        solution_design: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Create implementation guide using the Guide Creator agent"""
        system_prompt = self.templates.get_system_prompt(AgentType.GUIDE_CREATOR)

        prompt = f"""
ë¬¸ì œ ë¶„ì„:
{json.dumps(problem_analysis, indent=2, ensure_ascii=False)}

ìš”êµ¬ì‚¬í•­:
{requirements_text}

ì†”ë£¨ì…˜ ì„¤ê³„:
{json.dumps(solution_design, indent=2, ensure_ascii=False)}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œë°œìê°€ ë°”ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ìƒì„¸í•œ êµ¬í˜„ ê°€ì´ë“œë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë‹¤ìŒ êµ¬ì¡°ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:

1. í”„ë¡œì íŠ¸ ê°œìš”
2. ê°œë°œ í™˜ê²½ ì„¤ì •
3. ë‹¨ê³„ë³„ êµ¬í˜„ ê³„íš
4. ì£¼ìš” í•¨ìˆ˜/í´ë˜ìŠ¤ êµ¬í˜„
5. í…ŒìŠ¤íŠ¸ ë°©ë²•
6. ë°°í¬ ë° ìš´ì˜ ê°€ì´ë“œ

í•œê¸€ë¡œ ì‘ì„±í•˜ë˜, ì½”ë“œ ì˜ˆì œëŠ” ì‹¤ì œ ë™ì‘í•˜ëŠ” Python ì½”ë“œë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
"""

        try:
            response = await self.llm_manager.simple_completion(
                prompt,
                system_prompt,
                temperature=0.2
            )

            # ê°€ì´ë“œê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê¸°ë³¸ êµ¬ì¡° ì¶”ê°€
            if len(response) < 1000:
                return self._generate_implementation_guide_from_analysis(
                    problem_analysis, solution_design
                )

            return response

        except Exception as e:
            logger.error(f"Failed to create implementation guide - NO FALLBACK: {e}")
            # ë°”ë¡œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ ë¬¸ì œë¥¼ ì¦‰ì‹œ ì•Œë¦¼
            raise ValueError(f"Implementation guide creation failed: {e}")

    def _generate_implementation_guide_from_analysis(
        self,
        problem_analysis: Dict[str, Any],
        solution_design: Dict[str, Any]
    ) -> str:
        """ë¬¸ì œ ë¶„ì„ê³¼ ì†”ë£¨ì…˜ ì„¤ê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„ ê°€ì´ë“œ ìƒì„±"""
        category = problem_analysis.get("category", "GENERAL_PROBLEM_SOLVING")
        complexity = problem_analysis.get("complexity", "MEDIUM")
        tech_stack = solution_design.get("technology_stack", {})
        components = solution_design.get("architecture", {}).get("components", [])

        guide = f"""# {category.replace('_', ' ').title()} êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”
- **ë¬¸ì œ ìœ í˜•**: {category}
- **ë³µì¡ë„**: {complexity}
- **ì£¼ìš” ì–¸ì–´**: {tech_stack.get('primary_language', 'Python')}
- **ì˜ˆìƒ ê°œë°œ ê¸°ê°„**: {self._estimate_development_time(complexity)}

## ğŸ›  ê°œë°œ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
- Python 3.8 ì´ìƒ
- pip íŒ¨í‚¤ì§€ ê´€ë¦¬ì

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
pip install {' '.join(tech_stack.get('libraries', ['pandas', 'requests']))}
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°
```
project/
â”œâ”€â”€ {'/'.join(components[:3] if len(components) > 3 else components)}
{'â”œâ”€â”€ ' + '/'.join(components[3:]) if len(components) > 3 else ''}
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ tests/
    â””â”€â”€ test_main.py
```

## ğŸš€ ë‹¨ê³„ë³„ êµ¬í˜„ ê³„íš

### 1ë‹¨ê³„: ê¸°ë³¸ êµ¬ì¡° ì„¤ì •
```python
# {components[0] if components else 'main.py'}
import logging
import sys
from pathlib import Path

def setup_logging():
    \"\"\"ë¡œê¹… ì„¤ì •\"\"\"
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

def main():
    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"
    setup_logging()
    logging.info("í”„ë¡œê·¸ë¨ ì‹œì‘")

    try:
        # ë©”ì¸ ë¡œì§ êµ¬í˜„
        pass
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë°œìƒ: {{e}}")
        return False

    logging.info("í”„ë¡œê·¸ë¨ ì™„ë£Œ")
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
```

### 2ë‹¨ê³„: í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„
{self._generate_core_implementation(category, tech_stack)}

### 3ë‹¨ê³„: ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
```python
def handle_error(error: Exception, context: str = ""):
    \"\"\"í†µí•© ì—ëŸ¬ ì²˜ë¦¬\"\"\"
    logging.error(f"{{context}} ì˜¤ë¥˜: {{str(error)}}")
    # í•„ìš”ì‹œ ì—ëŸ¬ ìƒì„¸ ì •ë³´ ì €ì¥
    return None
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```python
# tests/test_main.py
import unittest
from main import main

class TestMain(unittest.TestCase):
    def test_main_execution(self):
        \"\"\"ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰ í…ŒìŠ¤íŠ¸\"\"\"
        result = main()
        self.assertTrue(result)

if __name__ == "__main__":
    unittest.main()
```

### ì‹¤í–‰ ë°©ë²•
```bash
python -m unittest tests/test_main.py -v
```

## ğŸ“¦ ë°°í¬ ë° ìš´ì˜

### requirements.txt ìƒì„±
```txt
{chr(10).join(tech_stack.get('libraries', ['pandas', 'requests']))}
```

### ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
cd "$(dirname "$0")"
python main.py
```

## ğŸ”§ ìœ ì§€ë³´ìˆ˜ ê°€ì´ë“œ

1. **ë¡œê·¸ í™•ì¸**: ì‹¤í–‰ ë¡œê·¸ë¥¼ ì •ê¸°ì ìœ¼ë¡œ ê²€í† 
2. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: ì²˜ë¦¬ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
3. **ì—ëŸ¬ ëŒ€ì‘**: ë¡œê·¸ì— ê¸°ë¡ëœ ì—ëŸ¬ íŒ¨í„´ ë¶„ì„
4. **ë°±ì—…**: ì¤‘ìš” ë°ì´í„° ì •ê¸° ë°±ì—…

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤
- Python ê³µì‹ ë¬¸ì„œ: https://docs.python.org/ko/3/
- {category} ê´€ë ¨ ëª¨ë²” ì‚¬ë¡€ ê²€ìƒ‰ ê¶Œì¥

ğŸ¤– **AI ìƒì„± ê°€ì´ë“œ**: ì‹¤ì œ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ìˆ˜ì •í•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”.
"""
        return guide

    def _estimate_development_time(self, complexity: str) -> str:
        """ë³µì¡ë„ì— ë”°ë¥¸ ê°œë°œ ì‹œê°„ ì¶”ì •"""
        time_estimates = {
            "LOW": "1-2ì£¼",
            "MEDIUM": "3-4ì£¼",
            "HIGH": "6-8ì£¼"
        }
        return time_estimates.get(complexity, "3-4ì£¼")

    def _generate_core_implementation(self, category: str, tech_stack: Dict[str, Any]) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ í•µì‹¬ êµ¬í˜„ ì˜ˆì œ ìƒì„±"""
        if category == "AUTOMATION":
            return """```python
def process_automation():
    \"\"\"ìë™í™” í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰\"\"\"
    import schedule
    import time

    def job():
        print("ìë™í™” ì‘ì—… ì‹¤í–‰ ì¤‘...")
        # ì‹¤ì œ ì‘ì—… ë¡œì§ êµ¬í˜„
        return True

    # ìŠ¤ì¼€ì¤„ ì„¤ì • (ë§¤ì¼ ì˜¤ì „ 9ì‹œ)
    schedule.every().day.at("09:00").do(job)

    while True:
        schedule.run_pending()
        time.sleep(60)
```"""
        elif category == "DATA_VISUALIZATION":
            return """```python
import matplotlib.pyplot as plt
import pandas as pd

def create_visualization(data):
    \"\"\"ë°ì´í„° ì‹œê°í™” ìƒì„±\"\"\"
    plt.figure(figsize=(10, 6))
    plt.plot(data['x'], data['y'])
    plt.title('ë°ì´í„° ì‹œê°í™”')
    plt.xlabel('Xì¶•')
    plt.ylabel('Yì¶•')
    plt.save('output.png')
    plt.show()
```"""
        else:
            return """```python
def core_function():
    \"\"\"í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„\"\"\"
    # ë°ì´í„° ì…ë ¥
    input_data = get_input_data()

    # ë°ì´í„° ì²˜ë¦¬
    processed_data = process_data(input_data)

    # ê²°ê³¼ ì¶œë ¥
    save_result(processed_data)
    return processed_data
```"""

    async def close(self):
        """Close LLM manager"""
        await self.llm_manager.close()


# Global service instances
_llm_manager: Optional[LLMManager] = None
_agent_service: Optional[LLMAgentService] = None


async def get_llm_manager() -> LLMManager:
    """Get the global LLM manager instance"""
    global _llm_manager
    
    if _llm_manager is None:
        _llm_manager = LLMManager()
    
    return _llm_manager


async def get_agent_service() -> LLMAgentService:
    """Get the global LLM agent service instance"""
    global _agent_service
    
    if _agent_service is None:
        llm_manager = await get_llm_manager()
        _agent_service = LLMAgentService(llm_manager)
    
    return _agent_service


async def cleanup_llm_services():
    """Clean up all global LLM services"""
    global _llm_manager, _agent_service
    
    if _agent_service:
        await _agent_service.close()
        _agent_service = None
    
    if _llm_manager:
        await _llm_manager.close()
        _llm_manager = None