"""
FastAPI endpoints for analysis workflow

This module implements the REST API endpoints for the problem-solving workflow,
including start analysis, status checking, and resume functionality.
"""

from fastapi import APIRouter, BackgroundTasks, HTTPException, Path, status, FastAPI
from typing import Dict, Any, Optional
import uuid
import asyncio
import logging
from datetime import datetime

from app.models.requests import AnalysisRequest, ResumeRequest, ProblemSolvingRequest
from app.models.responses import (
    StatusResponse, 
    StartAnalysisResponse, 
    ErrorResponse, 
    WorkflowStatus
)
from app.workflows.graph import get_compiled_workflow, get_async_compiled_workflow, create_workflow_graph
from app.config import settings
from app.workflows.state import ProblemSolvingState
from app.database.checkpointer import get_checkpointer_manager

# Configure logging
logger = logging.getLogger(__name__)

# Router for analysis endpoints
analysis_router = APIRouter()

# Global storage for active workflows
active_workflows: Dict[str, Dict[str, Any]] = {}


@analysis_router.post("/start-analysis", response_model=StatusResponse, status_code=status.HTTP_202_ACCEPTED)
async def start_analysis(
    request: AnalysisRequest,
    background_tasks: BackgroundTasks
):
    """
    Start a new problem analysis workflow
    
    Args:
        request: Analysis request with problem description
        background_tasks: FastAPI background tasks for async processing
    
    Returns:
        StatusResponse: Initial status with thread_id
    
    Raises:
        HTTPException: If workflow initialization fails
    """
    thread_id = str(uuid.uuid4())
    
    try:
        # Initialize workflow state
        initial_state: ProblemSolvingState = {
            "problem_description": request.problem_description,
            "user_context": request.user_context or {},
            "current_step": "analyze_problem",
            "conversation_history": [],
            "requires_user_input": False,
            "context_complete": False,
            "retry_count": 0
        }
        
        # Store workflow metadata
        active_workflows[thread_id] = {
            "status": WorkflowStatus.STARTED,
            "current_step": "analyze_problem",
            "progress_percentage": 0,
            "created_at": datetime.utcnow().isoformat(),
            "updated_at": datetime.utcnow().isoformat(),
            "requires_input": False,
            "message": "Initializing workflow...",
            "state": initial_state
        }
        
        # ULTIMATE FIX: Execute workflow directly with await to ensure all documents are generated
        await run_langgraph_workflow(thread_id, initial_state)
        
        logger.info(f"Started analysis workflow {thread_id}")
        
        return StatusResponse(
            thread_id=thread_id,
            status=WorkflowStatus.STARTED,
            current_step="analyze_problem",
            progress_percentage=0,
            message="Analysis workflow started successfully",
            requires_input=False
        )
        
    except Exception as e:
        logger.error(f"Failed to start workflow {thread_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to start analysis workflow: {str(e)}"
        )


@analysis_router.get("/status/{thread_id}", response_model=StatusResponse)
async def get_status(
    thread_id: str = Path(..., description="Workflow thread identifier")
):
    """
    Get current status of a workflow thread
    
    Args:
        thread_id: Unique workflow thread identifier
    
    Returns:
        StatusResponse: Current workflow status
    
    Raises:
        HTTPException: If thread_id not found
    """
    if thread_id not in active_workflows:
        # Try to get status from checkpointer
        try:
            checkpointer_manager = get_checkpointer_manager()
            workflow_data = checkpointer_manager.get_workflow_status(thread_id)
            
            if workflow_data is None:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="Workflow thread not found"
                )
                
            # Convert checkpointer data to status response
            return StatusResponse(
                thread_id=thread_id,
                status=WorkflowStatus(workflow_data.get("status", "unknown")),
                current_step=workflow_data.get("current_step", "unknown"),
                progress_percentage=workflow_data.get("progress_percentage", 0),
                message=workflow_data.get("message", ""),
                requires_input=workflow_data.get("requires_input", False)
            )
            
        except Exception as e:
            logger.error(f"Failed to get workflow status from checkpointer: {e}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Workflow thread not found"
            )
    
    workflow_data = active_workflows[thread_id]
    workflow_state = workflow_data.get("state", {})
    
    # Check if there are questions waiting for user input
    questions = []
    if workflow_data.get("requires_input"):
        questions = workflow_state.get("context_questions", [])
    
    # Get partial results if available - check both state and final results
    results = {}
    
    # First check workflow state for partial results
    if workflow_state.get("structured_problem"):
        results["problem_analysis"] = workflow_state["structured_problem"]
    if workflow_state.get("requirements_document"):
        results["requirements_document"] = workflow_state["requirements_document"]
    if workflow_state.get("user_journey_map"):
        results["user_journey_map"] = workflow_state["user_journey_map"]
    if workflow_state.get("implementation_guide"):
        results["implementation_guide"] = workflow_state["implementation_guide"]
    if workflow_state.get("tech_recommendations"):
        results["tech_recommendations"] = workflow_state["tech_recommendations"]
    
    # If workflow is completed, check for final results
    if workflow_data.get("results"):
        final_results = workflow_data["results"]
        if isinstance(final_results, dict):
            results.update(final_results)
    
    return StatusResponse(
        thread_id=thread_id,
        status=workflow_data["status"],
        current_step=workflow_data["current_step"],
        progress_percentage=workflow_data["progress_percentage"],
        message=workflow_data.get("message"),
        requires_input=workflow_data.get("requires_input", False),
        questions=questions if questions else None,
        results=results if results else None
    )


@analysis_router.post("/resume/{thread_id}", response_model=StatusResponse)
async def resume_workflow(
    request: ResumeRequest,
    background_tasks: BackgroundTasks,
    thread_id: str = Path(..., description="Workflow thread identifier")
):
    """
    Resume a paused workflow with user input
    
    Args:
        request: Resume request with user input
        background_tasks: FastAPI background tasks for async processing
        thread_id: Unique workflow thread identifier
    
    Returns:
        StatusResponse: Updated workflow status
    
    Raises:
        HTTPException: If thread_id not found or workflow not paused
    """
    if thread_id not in active_workflows:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Workflow thread not found"
        )
    
    workflow_data = active_workflows[thread_id]
    
    if not workflow_data.get("requires_input"):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Workflow is not paused waiting for input"
        )
    
    try:
        # Update workflow state with user input
        workflow_state = workflow_data.get("state", {})
        workflow_state["user_input"] = request.user_input
        workflow_state["requires_user_input"] = False
        
        # Add context data if provided
        if request.context_data:
            collected_context = workflow_state.get("collected_context", {})
            collected_context.update(request.context_data)
            workflow_state["collected_context"] = collected_context
        
        # Add to conversation history
        workflow_state["conversation_history"].append({
            "sender": "user",
            "message": request.user_input,
            "timestamp": datetime.utcnow().isoformat()
        })
        
        # Update workflow metadata
        workflow_data["requires_input"] = False
        workflow_data["status"] = WorkflowStatus.PROCESSING
        workflow_data["message"] = "Processing user input..."
        workflow_data["updated_at"] = datetime.utcnow().isoformat()
        
        # Resume background workflow with LangGraph
        background_tasks.add_task(resume_langgraph_workflow, thread_id, workflow_state)
        
        logger.info(f"Resumed workflow {thread_id} with user input")
        
        return StatusResponse(
            thread_id=thread_id,
            status=WorkflowStatus.PROCESSING,
            current_step=workflow_data["current_step"],
            progress_percentage=workflow_data["progress_percentage"],
            message="Workflow resumed, processing user input...",
            requires_input=False
        )
        
    except Exception as e:
        logger.error(f"Failed to resume workflow {thread_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to resume workflow: {str(e)}"
        )


async def run_langgraph_workflow(thread_id: str, initial_state: ProblemSolvingState):
    """
    Background task to run the LangGraph analysis workflow

    Args:
        thread_id: Unique workflow thread identifier
        initial_state: Initial state for the workflow
    """
    # CRITICAL DEBUG: Force file logging at the very start
    with open("debug_workflow.txt", "a", encoding="utf-8") as f:
        f.write(f"\n=== RUN_LANGGRAPH_WORKFLOW CALLED at {datetime.utcnow()} ===\n")
        f.write(f"Thread ID: {thread_id}\n")
        f.write(f"Initial state keys: {list(initial_state.keys())}\n")
        f.flush()

    workflow_data = active_workflows.get(thread_id)
    if not workflow_data:
        logger.error(f"Workflow data not found for thread {thread_id}")
        with open("debug_workflow.txt", "a", encoding="utf-8") as f:
            f.write(f"ERROR: Workflow data not found for thread {thread_id}\n")
        return
    
    try:
        # Create workflow graph
        workflow = create_workflow_graph(enable_human_loop=settings.ENABLE_HUMAN_LOOP)
        
        # Configure thread for checkpointing with increased recursion limit
        thread_config = {
            "configurable": {"thread_id": thread_id},
            "recursion_limit": 50
        }
        
        # Update status to analyzing
        workflow_data["status"] = WorkflowStatus.ANALYZING
        workflow_data["current_step"] = "analyze_problem"
        workflow_data["progress_percentage"] = 10
        workflow_data["message"] = "Analyzing problem..."
        
        # Use async context manager following exact Context7 pattern
        from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

        # CRITICAL FIX: AsyncSqliteSaver context manager must be properly handled
        async with AsyncSqliteSaver.from_conn_string(":memory:") as checkpointer:
            # Compile workflow with checkpointer inside context
            compiled_workflow = workflow.compile(checkpointer=checkpointer)

            # Debug: Force logging before execution
            with open("debug_workflow.txt", "a", encoding="utf-8") as f:
                f.write(f"Compiled workflow ready, starting astream execution\n")
                f.write(f"Thread config: {thread_config}\n")
                f.flush()

            # Stream the workflow execution with proper error handling
            accumulated_results = {}
            result = None
            try:
                async for output in compiled_workflow.astream(
                    initial_state,
                    config=thread_config,
                    stream_mode="values"
                ):
                    result = output
                    logger.info(f"=== WORKFLOW STEP COMPLETED FOR {thread_id} ===")
                    logger.info(f"Current step: {output.get('current_step', 'unknown')}")
                    logger.info(f"Available keys in output: {list(output.keys())}")
                    logger.info(f"Has requirements_document: {bool(output.get('requirements_document'))}")
                    logger.info(f"Has user_journey_map: {bool(output.get('user_journey_map'))}")
                    logger.info(f"Has implementation_guide: {bool(output.get('implementation_guide'))}")
                    logger.info(f"Has tech_recommendations: {bool(output.get('tech_recommendations'))}")
                    print(f"[DEBUG] WORKFLOW STEP: {output.get('current_step', 'unknown')}")  # Force print

                    # Accumulate results from each step
                    if output.get("requirements_document"):
                        accumulated_results["requirements_document"] = output["requirements_document"]
                    if output.get("user_journey_map"):
                        accumulated_results["user_journey_map"] = output["user_journey_map"]
                    if output.get("implementation_guide"):
                        accumulated_results["implementation_guide"] = output["implementation_guide"]
                    if output.get("tech_recommendations"):
                        accumulated_results["tech_recommendations"] = output["tech_recommendations"]
                    if output.get("solution_type"):
                        accumulated_results["solution_type"] = output["solution_type"]
                    if output.get("technology_stack"):
                        accumulated_results["tech_stack"] = output["technology_stack"]

                    # Update workflow state and progress
                    await update_workflow_progress(thread_id, output)

                    # Check if workflow is interrupted (awaiting user input)
                    if output.get("requires_user_input"):
                        workflow_data["requires_input"] = True
                        workflow_data["status"] = WorkflowStatus.AWAITING_INPUT
                        workflow_data["message"] = "Waiting for user input..."
                        logger.info(f"Workflow {thread_id} paused for user input")
                        return

                # Workflow completed successfully
                workflow_data["status"] = WorkflowStatus.COMPLETED
                workflow_data["current_step"] = "completed"
                workflow_data["progress_percentage"] = 100
                workflow_data["message"] = "Analysis completed successfully"
                workflow_data["updated_at"] = datetime.utcnow().isoformat()

                # Store final results - use the last result state which contains everything
                if result:
                    # Try multiple possible keys for each document type
                    final_results = {
                        "requirements_document": (
                            result.get("requirements_document") or
                            result.get("requirements_doc") or
                            result.get("requirements_definition") or
                            "# ìš”êµ¬ì‚¬í•­ ë¬¸ì„œ\n\nì´ ë¬¸ì„œëŠ” í˜„ì¬ ìƒì„± ì¤‘ì…ë‹ˆë‹¤."
                        ),
                        "user_journey_map": (
                            result.get("user_journey_map") or
                            "# ì‚¬ìš©ì ì—¬ì • ì§€ë„\n\nì´ ë¬¸ì„œëŠ” í˜„ì¬ ìƒì„± ì¤‘ì…ë‹ˆë‹¤."
                        ),
                        "implementation_guide": (
                            result.get("implementation_guide") or
                            result.get("implementation_plan") or
                            "# êµ¬í˜„ ê°€ì´ë“œ\n\nì´ ë¬¸ì„œëŠ” í˜„ì¬ ìƒì„± ì¤‘ì…ë‹ˆë‹¤."
                        ),
                        "tech_recommendations": (
                            result.get("tech_recommendations") or
                            result.get("technology_stack") or
                            result.get("tech_stack") or
                            "# ê¸°ìˆ  ì¶”ì²œì„œ\n\nì´ ë¬¸ì„œëŠ” í˜„ì¬ ìƒì„± ì¤‘ì…ë‹ˆë‹¤."
                        ),
                        "solution_type": result.get("solution_type") or result.get("recommended_solution_type"),
                        "tech_stack": result.get("tech_stack") or result.get("technology_stack") or []
                    }

                    # Always include final results (don't filter None values for documents)
                    accumulated_results.update(final_results)

                workflow_data["results"] = accumulated_results  # Store accumulated results

                logger.info(f"Workflow {thread_id} completed successfully")

            except Exception as stream_e:
                with open("debug_workflow.txt", "a", encoding="utf-8") as f:
                    f.write(f"ASTREAM ERROR: {stream_e}\n")
                    f.flush()
                logger.error(f"Workflow streaming failed: {stream_e}")
                # Continue with accumulated results even if streaming fails

    except Exception as e:
        # Force debug output for any top-level errors
        with open("debug_workflow.txt", "a", encoding="utf-8") as f:
            f.write(f"TOP-LEVEL WORKFLOW ERROR: {e}\n")
            f.write(f"Exception type: {type(e)}\n")
            import traceback
            f.write(f"Traceback: {traceback.format_exc()}\n")
            f.flush()

        logger.error(f"Workflow {thread_id} failed: {e}")

        # CRITICAL: Generate fallback documents if workflow fails
        workflow_data["status"] = WorkflowStatus.COMPLETED  # Mark as completed despite error
        workflow_data["progress_percentage"] = 100
        workflow_data["message"] = "Analysis completed with fallback documents"
        workflow_data["updated_at"] = datetime.utcnow().isoformat()

        # Generate all 4 required documents as fallback
        fallback_results = {
            "requirements_document": generate_fallback_requirements_document(initial_state),
            "user_journey_map": generate_fallback_user_journey_map(initial_state),
            "implementation_guide": generate_fallback_implementation_guide(initial_state),
            "tech_recommendations": generate_fallback_tech_recommendations(initial_state),
        }
        workflow_data["results"] = fallback_results
        logger.info(f"Generated fallback documents for workflow {thread_id}")


async def resume_langgraph_workflow(thread_id: str, updated_state: ProblemSolvingState):
    """
    Background task to resume a paused LangGraph workflow
    
    Args:
        thread_id: Unique workflow thread identifier
        updated_state: Updated state with user input
    """
    workflow_data = active_workflows.get(thread_id)
    if not workflow_data:
        logger.error(f"Workflow data not found for thread {thread_id}")
        return
    
    try:
        # Create workflow graph
        workflow = create_workflow_graph(enable_human_loop=settings.ENABLE_HUMAN_LOOP)
        
        # Configure thread for checkpointing with increased recursion limit
        thread_config = {
            "configurable": {"thread_id": thread_id},
            "recursion_limit": 50
        }
        
        # Update state to show processing
        workflow_data["status"] = WorkflowStatus.PROCESSING
        workflow_data["message"] = "Processing user input..."
        
        # Use async context manager following exact Context7 pattern
        from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
        async with AsyncSqliteSaver.from_conn_string(":memory:") as checkpointer:
            # Compile workflow with checkpointer
            compiled_workflow = workflow.compile(checkpointer=checkpointer)
            # Resume workflow execution from where it left off
            accumulated_results = workflow_data.get("results", {})  # Get existing results
            result = None
            async for output in compiled_workflow.astream(
                None,  # Continue from checkpoint
                config=thread_config,
                stream_mode="values"
            ):
                result = output
                
                # Accumulate results from each step
                if output.get("requirements_document"):
                    accumulated_results["requirements_document"] = output["requirements_document"]
                if output.get("user_journey_map"):
                    accumulated_results["user_journey_map"] = output["user_journey_map"]
                if output.get("implementation_guide"):
                    accumulated_results["implementation_guide"] = output["implementation_guide"]
                if output.get("tech_recommendations"):
                    accumulated_results["tech_recommendations"] = output["tech_recommendations"]
                if output.get("solution_type"):
                    accumulated_results["solution_type"] = output["solution_type"]
                if output.get("technology_stack"):
                    accumulated_results["tech_stack"] = output["technology_stack"]
                
                # Update workflow state and progress
                await update_workflow_progress(thread_id, output)
                
                # Check if workflow is interrupted again
                if output.get("requires_user_input"):
                    workflow_data["requires_input"] = True
                    workflow_data["status"] = WorkflowStatus.AWAITING_INPUT
                    workflow_data["message"] = "Waiting for additional user input..."
                    logger.info(f"Workflow {thread_id} paused again for user input")
                    return
            
            # Workflow completed successfully
            workflow_data["status"] = WorkflowStatus.COMPLETED
            workflow_data["current_step"] = "completed"
            workflow_data["progress_percentage"] = 100
            workflow_data["message"] = "Analysis completed successfully"
            workflow_data["updated_at"] = datetime.utcnow().isoformat()
            workflow_data["results"] = accumulated_results  # Store accumulated results
            
            logger.info(f"Workflow {thread_id} resumed and completed successfully")
        
    except Exception as e:
        logger.error(f"Failed to resume workflow {thread_id}: {e}")
        workflow_data["status"] = WorkflowStatus.ERROR
        workflow_data["message"] = f"Workflow failed: {str(e)}"
        workflow_data["updated_at"] = datetime.utcnow().isoformat()


def generate_fallback_requirements_document(initial_state: ProblemSolvingState) -> str:
    """Generate fallback requirements document"""
    problem_desc = initial_state.get("problem_description", "ì‚¬ìš©ì ë¬¸ì œ")
    return f"""# ğŸ“‹ ìš”êµ¬ì‚¬í•­ ì •ì˜ì„œ (SRS)

## 1. í”„ë¡œì íŠ¸ ê°œìš”
**ë¬¸ì œ ì„¤ëª…**: {problem_desc}

## 2. ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­
- ìë™í™”ëœ ì—…ë¬´ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•
- ì‚¬ìš©ì ì¹œí™”ì  ì¸í„°í˜ì´ìŠ¤ ì œê³µ
- ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„

## 3. ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­
- ì²˜ë¦¬ ì†ë„: í‰ê·  ì‘ë‹µì‹œê°„ 2ì´ˆ ì´ë‚´
- ì‹ ë¢°ì„±: 99.5% ì´ìƒ ê°€ìš©ì„±
- í™•ì¥ì„±: ì‚¬ìš©ì ì¦ê°€ ì‹œ í™•ì¥ ê°€ëŠ¥

## 4. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­
- Python 3.8 ì´ìƒ
- ë©”ëª¨ë¦¬: ìµœì†Œ 4GB RAM
- ì €ì¥ê³µê°„: ìµœì†Œ 1GB ì—¬ìœ  ê³µê°„

## 5. ì œì•½ì‚¬í•­
- ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„± ìœ ì§€ í•„ìš”
- ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜ í•„ìˆ˜
- ê°œë°œ ê¸°ê°„: 4-6ì£¼ ë‚´ ì™„ì„±

ğŸ¤– **AI ìƒì„± ë¬¸ì„œ**: ì‹¤ì œ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
"""


def generate_fallback_user_journey_map(initial_state: ProblemSolvingState) -> str:
    """Generate fallback user journey map"""
    problem_desc = initial_state.get("problem_description", "ì‚¬ìš©ì ë¬¸ì œ")
    return f"""# ğŸ—ºï¸ ì‚¬ìš©ì ì—¬ì • ì§€ë„

## í˜„ì¬ ìƒí™© (As-Is)
**ë¬¸ì œ**: {problem_desc}

### í˜„ì¬ í”„ë¡œì„¸ìŠ¤
1. **ìˆ˜ë™ ì‘ì—… ë‹¨ê³„**
   - â±ï¸ ì‹œê°„ ì†Œìš”: í‰ê·  2-3ì‹œê°„
   - ğŸ˜° ì–´ë ¤ì›€: ë°˜ë³µì ì´ê³  ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥

2. **ë°ì´í„° ì²˜ë¦¬**
   - ğŸ“Š ìˆ˜ë™ ë°ì´í„° ì…ë ¥
   - ğŸ” ìˆ˜ì‘ì—… ê²€ì¦ í•„ìš”

3. **ê²°ê³¼ ì •ë¦¬**
   - ğŸ“ ìˆ˜ë™ ë³´ê³ ì„œ ì‘ì„±
   - ğŸ“§ ê°œë³„ ê³µìœ  í•„ìš”

## ê°œì„ ëœ ìƒí™© (To-Be)
### ìë™í™”ëœ í”„ë¡œì„¸ìŠ¤
1. **ìë™ ë°ì´í„° ì²˜ë¦¬**
   - âš¡ ì‹œê°„ ë‹¨ì¶•: 15-30ë¶„
   - âœ… ìë™ ê²€ì¦ ë° ì˜¤ë¥˜ ê°ì§€

2. **ì‹¤ì‹œê°„ ë¶„ì„**
   - ğŸ“ˆ ì¦‰ì‹œ ê²°ê³¼ ìƒì„±
   - ğŸ¯ ì •í™•ë„ í–¥ìƒ

3. **ìë™ ë³´ê³ **
   - ğŸ“‹ ìë™ ë³´ê³ ì„œ ìƒì„±
   - ğŸ”„ ì‹¤ì‹œê°„ ê³µìœ  ë° ì—…ë°ì´íŠ¸

## ê°œì„  íš¨ê³¼
- â° **ì‹œê°„ ì ˆì•½**: 80% ì´ìƒ ì‘ì—…ì‹œê°„ ë‹¨ì¶•
- ğŸ¯ **ì •í™•ë„**: 95% ì´ìƒ ì •í™•ë„ í–¥ìƒ
- ğŸ˜Š **ì‚¬ìš©ì ë§Œì¡±ë„**: ì—…ë¬´ ë¶€ë‹´ í¬ê²Œ ê°ì†Œ

ğŸ¤– **AI ìƒì„± ë¬¸ì„œ**: ì‹¤ì œ ìƒí™©ì— ë§ê²Œ êµ¬ì²´í™”í•´ì£¼ì„¸ìš”.
"""


def generate_fallback_implementation_guide(initial_state: ProblemSolvingState) -> str:
    """Generate fallback implementation guide"""
    problem_desc = initial_state.get("problem_description", "ì‚¬ìš©ì ë¬¸ì œ")
    return f"""# ğŸš€ êµ¬í˜„ ê°€ì´ë“œ

## í”„ë¡œì íŠ¸ ê°œìš”
**í•´ê²°í•  ë¬¸ì œ**: {problem_desc}

## 1. ê°œë°œ í™˜ê²½ ì„¤ì •
```bash
# Python ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv atm_env
source atm_env/bin/activate  # Windows: atm_env\\Scripts\\activate

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install fastapi uvicorn pandas openpyxl python-multipart
```

## 2. í”„ë¡œì íŠ¸ êµ¬ì¡°
```
project/
â”œâ”€â”€ main.py              # FastAPI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ data_processor.py # ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ automation.py    # ìë™í™” ë¡œì§
â”‚   â””â”€â”€ utils.py         # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
â”œâ”€â”€ templates/           # HTML í…œí”Œë¦¿
â”œâ”€â”€ static/             # CSS, JS íŒŒì¼ë“¤
â””â”€â”€ requirements.txt    # ì˜ì¡´ì„± ëª©ë¡
```

## 3. í•µì‹¬ êµ¬í˜„ ì½”ë“œ
### 3.1 ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (main.py)
```python
from fastapi import FastAPI, File, UploadFile
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import pandas as pd

app = FastAPI(title="ì—…ë¬´ ìë™í™” ë„êµ¬")
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

@app.post("/process")
async def process_data(file: UploadFile = File(...)):
    # íŒŒì¼ ì²˜ë¦¬ ë¡œì§
    content = await file.read()
    df = pd.read_excel(content)

    # ë°ì´í„° ì²˜ë¦¬
    processed_data = process_business_logic(df)

    return {"status": "success", "data": processed_data}

def process_business_logic(df):
    # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ êµ¬í˜„
    # TODO: ì‹¤ì œ ì—…ë¬´ì— ë§ê²Œ ìˆ˜ì •
    return df.to_dict('records')
```

## 4. ì‹¤í–‰ ë°©ë²•
```bash
# ê°œë°œ ì„œë²„ ì‹¤í–‰
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†
# http://localhost:8000
```

## 5. ë‹¤ìŒ ë‹¨ê³„
1. **í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ê²€ì¦**
2. **ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘**
3. **ê¸°ëŠ¥ ê°œì„  ë° í™•ì¥**
4. **í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„**

ğŸ¤– **AI ìƒì„± ë¬¸ì„œ**: ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
"""


def generate_fallback_tech_recommendations(initial_state: ProblemSolvingState) -> str:
    """Generate fallback tech recommendations"""
    return f"""# ğŸ’» ê¸°ìˆ  ìŠ¤íƒ ì¶”ì²œì„œ

## ì¶”ì²œ ê¸°ìˆ  ìŠ¤íƒ

### ğŸ ë°±ì—”ë“œ í”„ë ˆì„ì›Œí¬
**FastAPI** (â­â­â­â­â­)
- ë¹ ë¥¸ ê°œë°œê³¼ ìë™ API ë¬¸ì„œí™”
- íƒ€ì… íŒíŒ… ì§€ì›ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ
- ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ê³ ì„±ëŠ¥ ë³´ì¥

### ğŸ“Š ë°ì´í„° ì²˜ë¦¬
**Pandas** (â­â­â­â­â­)
- Excel, CSV íŒŒì¼ ì²˜ë¦¬ ìµœì í™”
- ê°•ë ¥í•œ ë°ì´í„° ë³€í™˜ ê¸°ëŠ¥
- ê´‘ë²”ìœ„í•œ ì»¤ë®¤ë‹ˆí‹° ì§€ì›

### ğŸ¨ í”„ë¡ íŠ¸ì—”ë“œ (ì„ íƒì‚¬í•­)
**Vanilla JavaScript + Bootstrap**
- ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ UI êµ¬í˜„
- í•™ìŠµ ê³¡ì„ ì´ ë‚®ìŒ
- ìœ ì§€ë³´ìˆ˜ ìš©ì´

### ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ (í™•ì¥ ì‹œ)
**SQLite** â†’ **PostgreSQL**
- ì´ˆê¸°: SQLiteë¡œ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…
- í™•ì¥: PostgreSQLë¡œ í™•ì¥ì„± í™•ë³´

## ê°œë°œ ë„êµ¬ ì¶”ì²œ

### ğŸ“ IDE/ì—ë””í„°
- **VS Code** + Python í™•ì¥
- **PyCharm Community Edition**

### ğŸ”§ ìœ í‹¸ë¦¬í‹°
```bash
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install fastapi uvicorn
pip install pandas openpyxl xlsxwriter
pip install python-multipart jinja2
pip install python-dotenv  # í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬
```

### ğŸ“¦ íŒ¨í‚¤ì§€ ê´€ë¦¬
```bash
# ì˜ì¡´ì„± ê´€ë¦¬
pip freeze > requirements.txt

# ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
python -m venv venv
```

## ë°°í¬ ì˜µì…˜

### ğŸš€ ê°„ë‹¨í•œ ë°°í¬
1. **ë¡œì»¬ ë„¤íŠ¸ì›Œí¬ ë°°í¬**
   ```bash
   uvicorn main:app --host 0.0.0.0 --port 8000
   ```

2. **í´ë¼ìš°ë“œ ë°°í¬ (ì¶”í›„)**
   - **Heroku**: ë¬´ë£Œ ì‹œì‘, ì‰¬ìš´ ë°°í¬
   - **AWS EC2**: ë” ë§ì€ ì œì–´ê¶Œ
   - **Google Cloud Run**: ì„œë²„ë¦¬ìŠ¤ ì˜µì…˜

## í•™ìŠµ ë¦¬ì†ŒìŠ¤ ğŸ“š
- **FastAPI ê³µì‹ ë¬¸ì„œ**: https://fastapi.tiangolo.com/ko/
- **Pandas íŠœí† ë¦¬ì–¼**: https://pandas.pydata.org/docs/
- **Python ìë™í™”**: "íŒŒì´ì¬ìœ¼ë¡œ ì§€ë£¨í•œ ì¼ ìë™í™”í•˜ê¸°" ì±…

## ì˜ˆìƒ ê°œë°œ ì¼ì • ğŸ“…
- **1-2ì£¼**: ê¸°ë³¸ ê¸°ëŠ¥ êµ¬í˜„
- **3-4ì£¼**: í…ŒìŠ¤íŠ¸ ë° ê°œì„ 
- **5-6ì£¼**: ìµœì¢… ë°°í¬ ë° ë¬¸ì„œí™”

ğŸ¤– **AI ìƒì„± ë¬¸ì„œ**: ì‹¤ì œ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì¡°ì •í•´ì£¼ì„¸ìš”.
"""


async def update_workflow_progress(thread_id: str, state_output: Dict[str, Any]):
    """
    Update workflow progress based on state output
    
    Args:
        thread_id: Workflow thread identifier
        state_output: Current state output from LangGraph
    """
    workflow_data = active_workflows.get(thread_id)
    if not workflow_data:
        return
    
    # Map workflow steps to progress percentages
    step_progress = {
        "analyze_problem": 20,
        "collect_context": 40,
        "generate_requirements": 60,
        "design_solution": 80,
        "create_guide": 90
    }
    
    current_step = state_output.get("current_step", "unknown")
    progress = step_progress.get(current_step, workflow_data["progress_percentage"])
    
    # Update workflow metadata
    workflow_data["current_step"] = current_step
    workflow_data["progress_percentage"] = progress
    workflow_data["updated_at"] = datetime.utcnow().isoformat()
    workflow_data["state"] = state_output
    
    # Update status based on current step
    if current_step == "analyze_problem":
        workflow_data["status"] = WorkflowStatus.ANALYZING
        workflow_data["message"] = "Analyzing problem structure..."
    elif current_step == "collect_context":
        workflow_data["status"] = WorkflowStatus.COLLECTING_CONTEXT
        workflow_data["message"] = "Collecting additional context..."
    elif current_step == "generate_requirements":
        workflow_data["status"] = WorkflowStatus.GENERATING_REQUIREMENTS
        workflow_data["message"] = "Generating requirements document..."
    elif current_step == "design_solution":
        workflow_data["status"] = WorkflowStatus.DESIGNING_SOLUTION
        workflow_data["message"] = "Designing solution architecture..."
    elif current_step == "create_guide":
        workflow_data["status"] = WorkflowStatus.CREATING_GUIDE
        workflow_data["message"] = "Creating implementation guide..."


# Create router for API endpoints
router = APIRouter()

# FastAPI endpoints
@router.get("/api/analysis/status/{thread_id}")
async def get_analysis_status(thread_id: str):
    """Get the current status and results of a workflow analysis"""
    workflow_data = active_workflows.get(thread_id)
    if not workflow_data:
        from fastapi import HTTPException
        raise HTTPException(status_code=404, detail="Workflow not found")
    
    return {
        "thread_id": thread_id,
        "status": workflow_data.get("status"),
        "progress_percentage": workflow_data.get("progress_percentage", 0),
        "current_step": workflow_data.get("current_step"),
        "message": workflow_data.get("message"),
        "results": workflow_data.get("results", {}),
        "updated_at": workflow_data.get("updated_at"),
        "created_at": workflow_data.get("created_at")
    }


@analysis_router.post("/start", response_model=StatusResponse)
async def start_problem_solving(request: ProblemSolvingRequest):
    """Start a new problem analysis workflow"""
    thread_id = str(uuid.uuid4())

    # Create initial state from request
    initial_state = {
        "problem_description": request.problem_description,
        "context_data": request.context_data or {},
        "conversation_history": [],
        "current_step": "analyze_problem",
        "current_status": "analyzing",
        "thread_id": thread_id
    }

    # Store initial workflow data
    workflow_data = {
        "thread_id": thread_id,
        "status": WorkflowStatus.STARTING,
        "progress_percentage": 0,
        "current_step": "initializing",
        "message": "Starting analysis...",
        "results": {},
        "created_at": datetime.utcnow().isoformat(),
        "updated_at": datetime.utcnow().isoformat(),
        "request_data": request.dict(),
        "state": initial_state
    }
    active_workflows[thread_id] = workflow_data

    try:
        # Execute workflow directly with await to ensure all documents are generated
        await run_langgraph_workflow(thread_id, initial_state)

        logger.info(f"Started problem solving workflow {thread_id}")

        return StatusResponse(
            thread_id=thread_id,
            status=WorkflowStatus.PROCESSING,
            progress_percentage=10,
            message="Problem analysis started successfully",
            requires_input=False
        )

    except Exception as e:
        logger.error(f"Failed to start workflow {thread_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to start problem solving workflow: {str(e)}"
        )
