"""
FastAPI endpoints for analysis workflow

This module implements the REST API endpoints for the problem-solving workflow,
including start analysis, status checking, and resume functionality.
"""

from fastapi import APIRouter, BackgroundTasks, HTTPException, Path, status, FastAPI
from typing import Dict, Any, Optional
import uuid
import asyncio
import logging
from datetime import datetime

from app.models.requests import AnalysisRequest, ResumeRequest, ProblemSolvingRequest
from app.models.responses import (
    StatusResponse, 
    StartAnalysisResponse, 
    ErrorResponse, 
    WorkflowStatus
)
from app.workflows.graph import get_compiled_workflow, get_async_compiled_workflow, create_workflow_graph
from app.config import settings
from app.workflows.state import ProblemSolvingState
from app.database.checkpointer import get_checkpointer_manager

# Configure logging
logger = logging.getLogger(__name__)

# Router for analysis endpoints
analysis_router = APIRouter()

# Global storage for active workflows
active_workflows: Dict[str, Dict[str, Any]] = {}


@analysis_router.post("/start-analysis", response_model=StatusResponse, status_code=status.HTTP_202_ACCEPTED)
async def start_analysis(
    request: AnalysisRequest,
    background_tasks: BackgroundTasks
):
    """
    Start a new problem analysis workflow
    
    Args:
        request: Analysis request with problem description
        background_tasks: FastAPI background tasks for async processing
    
    Returns:
        StatusResponse: Initial status with thread_id
    
    Raises:
        HTTPException: If workflow initialization fails
    """
    thread_id = str(uuid.uuid4())
    
    try:
        # Initialize workflow state
        initial_state: ProblemSolvingState = {
            "problem_description": request.problem_description,
            "user_context": request.user_context or {},
            "current_step": "analyze_problem",
            "conversation_history": [],
            "requires_user_input": False,
            "context_complete": False,
            "retry_count": 0
        }
        
        # Store workflow metadata
        active_workflows[thread_id] = {
            "status": WorkflowStatus.STARTED,
            "current_step": "analyze_problem",
            "progress_percentage": 0,
            "created_at": datetime.utcnow().isoformat(),
            "updated_at": datetime.utcnow().isoformat(),
            "requires_input": False,
            "message": "Initializing workflow...",
            "state": initial_state
        }
        
        # ULTIMATE FIX: Execute workflow directly with await to ensure all documents are generated
        await run_langgraph_workflow(thread_id, initial_state)
        
        logger.info(f"Started analysis workflow {thread_id}")
        
        return StatusResponse(
            thread_id=thread_id,
            status=WorkflowStatus.STARTED,
            current_step="analyze_problem",
            progress_percentage=0,
            message="Analysis workflow started successfully",
            requires_input=False
        )
        
    except Exception as e:
        logger.error(f"Failed to start workflow {thread_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to start analysis workflow: {str(e)}"
        )


@analysis_router.get("/status/{thread_id}", response_model=StatusResponse)
async def get_status(
    thread_id: str = Path(..., description="Workflow thread identifier")
):
    """
    Get current status of a workflow thread
    
    Args:
        thread_id: Unique workflow thread identifier
    
    Returns:
        StatusResponse: Current workflow status
    
    Raises:
        HTTPException: If thread_id not found
    """
    if thread_id not in active_workflows:
        # Try to get status from checkpointer
        try:
            checkpointer_manager = get_checkpointer_manager()
            workflow_data = checkpointer_manager.get_workflow_status(thread_id)
            
            if workflow_data is None:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="Workflow thread not found"
                )
                
            # Convert checkpointer data to status response
            return StatusResponse(
                thread_id=thread_id,
                status=WorkflowStatus(workflow_data.get("status", "unknown")),
                current_step=workflow_data.get("current_step", "unknown"),
                progress_percentage=workflow_data.get("progress_percentage", 0),
                message=workflow_data.get("message", ""),
                requires_input=workflow_data.get("requires_input", False)
            )
            
        except Exception as e:
            logger.error(f"Failed to get workflow status from checkpointer: {e}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Workflow thread not found"
            )
    
    workflow_data = active_workflows[thread_id]
    workflow_state = workflow_data.get("state", {})
    
    # Check if there are questions waiting for user input
    questions = []
    if workflow_data.get("requires_input"):
        questions = workflow_state.get("context_questions", [])
    
    # Get partial results if available - check both state and final results
    results = {}
    
    # First check workflow state for partial results
    if workflow_state.get("structured_problem"):
        results["problem_analysis"] = workflow_state["structured_problem"]
    if workflow_state.get("requirements_document"):
        results["requirements_document"] = workflow_state["requirements_document"]
    if workflow_state.get("user_journey_map"):
        results["user_journey_map"] = workflow_state["user_journey_map"]
    if workflow_state.get("implementation_guide"):
        results["implementation_guide"] = workflow_state["implementation_guide"]
    if workflow_state.get("tech_recommendations"):
        results["tech_recommendations"] = workflow_state["tech_recommendations"]
    
    # If workflow is completed, check for final results
    if workflow_data.get("results"):
        final_results = workflow_data["results"]
        if isinstance(final_results, dict):
            results.update(final_results)
    
    return StatusResponse(
        thread_id=thread_id,
        status=workflow_data["status"],
        current_step=workflow_data["current_step"],
        progress_percentage=workflow_data["progress_percentage"],
        message=workflow_data.get("message"),
        requires_input=workflow_data.get("requires_input", False),
        questions=questions if questions else None,
        results=results if results else None
    )


@analysis_router.post("/resume/{thread_id}", response_model=StatusResponse)
async def resume_workflow(
    request: ResumeRequest,
    background_tasks: BackgroundTasks,
    thread_id: str = Path(..., description="Workflow thread identifier")
):
    """
    Resume a paused workflow with user input
    
    Args:
        request: Resume request with user input
        background_tasks: FastAPI background tasks for async processing
        thread_id: Unique workflow thread identifier
    
    Returns:
        StatusResponse: Updated workflow status
    
    Raises:
        HTTPException: If thread_id not found or workflow not paused
    """
    if thread_id not in active_workflows:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Workflow thread not found"
        )
    
    workflow_data = active_workflows[thread_id]
    
    if not workflow_data.get("requires_input"):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Workflow is not paused waiting for input"
        )
    
    try:
        # Update workflow state with user input
        workflow_state = workflow_data.get("state", {})
        workflow_state["user_input"] = request.user_input
        workflow_state["requires_user_input"] = False
        
        # Add context data if provided
        if request.context_data:
            collected_context = workflow_state.get("collected_context", {})
            collected_context.update(request.context_data)
            workflow_state["collected_context"] = collected_context
        
        # Add to conversation history
        workflow_state["conversation_history"].append({
            "sender": "user",
            "message": request.user_input,
            "timestamp": datetime.utcnow().isoformat()
        })
        
        # Update workflow metadata
        workflow_data["requires_input"] = False
        workflow_data["status"] = WorkflowStatus.PROCESSING
        workflow_data["message"] = "Processing user input..."
        workflow_data["updated_at"] = datetime.utcnow().isoformat()
        
        # Resume background workflow with LangGraph
        background_tasks.add_task(resume_langgraph_workflow, thread_id, workflow_state)
        
        logger.info(f"Resumed workflow {thread_id} with user input")
        
        return StatusResponse(
            thread_id=thread_id,
            status=WorkflowStatus.PROCESSING,
            current_step=workflow_data["current_step"],
            progress_percentage=workflow_data["progress_percentage"],
            message="Workflow resumed, processing user input...",
            requires_input=False
        )
        
    except Exception as e:
        logger.error(f"Failed to resume workflow {thread_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to resume workflow: {str(e)}"
        )


async def run_langgraph_workflow(thread_id: str, initial_state: ProblemSolvingState):
    """
    Background task to run the LangGraph analysis workflow

    Args:
        thread_id: Unique workflow thread identifier
        initial_state: Initial state for the workflow
    """
    # CRITICAL DEBUG: Force file logging at the very start
    with open("debug_workflow.txt", "a", encoding="utf-8") as f:
        f.write(f"\n=== RUN_LANGGRAPH_WORKFLOW CALLED at {datetime.utcnow()} ===\n")
        f.write(f"Thread ID: {thread_id}\n")
        f.write(f"Initial state keys: {list(initial_state.keys())}\n")
        f.flush()

    workflow_data = active_workflows.get(thread_id)
    if not workflow_data:
        logger.error(f"Workflow data not found for thread {thread_id}")
        with open("debug_workflow.txt", "a", encoding="utf-8") as f:
            f.write(f"ERROR: Workflow data not found for thread {thread_id}\n")
        return
    
    try:
        # Create workflow graph
        workflow = create_workflow_graph(enable_human_loop=settings.ENABLE_HUMAN_LOOP)
        
        # Configure thread for checkpointing with increased recursion limit
        thread_config = {
            "configurable": {"thread_id": thread_id},
            "recursion_limit": 50
        }
        
        # Update status to analyzing
        workflow_data["status"] = WorkflowStatus.ANALYZING
        workflow_data["current_step"] = "analyze_problem"
        workflow_data["progress_percentage"] = 10
        workflow_data["message"] = "Analyzing problem..."
        
        # Use async context manager following exact Context7 pattern
        from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

        # CRITICAL FIX: AsyncSqliteSaver context manager must be properly handled
        async with AsyncSqliteSaver.from_conn_string(":memory:") as checkpointer:
            # Compile workflow with checkpointer inside context
            compiled_workflow = workflow.compile(checkpointer=checkpointer)

            # Debug: Force logging before execution
            with open("debug_workflow.txt", "a", encoding="utf-8") as f:
                f.write(f"Compiled workflow ready, starting astream execution\n")
                f.write(f"Thread config: {thread_config}\n")
                f.flush()

            # Stream the workflow execution with proper error handling
            accumulated_results = {}
            result = None
            try:
                async for output in compiled_workflow.astream(
                    initial_state,
                    config=thread_config,
                    stream_mode="values"
                ):
                    result = output
                    logger.info(f"=== WORKFLOW STEP COMPLETED FOR {thread_id} ===")
                    logger.info(f"Current step: {output.get('current_step', 'unknown')}")
                    logger.info(f"Available keys in output: {list(output.keys())}")
                    logger.info(f"Has requirements_document: {bool(output.get('requirements_document'))}")
                    logger.info(f"Has user_journey_map: {bool(output.get('user_journey_map'))}")
                    logger.info(f"Has implementation_guide: {bool(output.get('implementation_guide'))}")
                    logger.info(f"Has tech_recommendations: {bool(output.get('tech_recommendations'))}")
                    print(f"[DEBUG] WORKFLOW STEP: {output.get('current_step', 'unknown')}")  # Force print

                    # Accumulate results from each step
                    if output.get("requirements_document"):
                        accumulated_results["requirements_document"] = output["requirements_document"]
                    if output.get("user_journey_map"):
                        accumulated_results["user_journey_map"] = output["user_journey_map"]
                    if output.get("implementation_guide"):
                        accumulated_results["implementation_guide"] = output["implementation_guide"]
                    if output.get("tech_recommendations"):
                        accumulated_results["tech_recommendations"] = output["tech_recommendations"]
                    if output.get("solution_type"):
                        accumulated_results["solution_type"] = output["solution_type"]
                    if output.get("technology_stack"):
                        accumulated_results["tech_stack"] = output["technology_stack"]

                    # Update workflow state and progress
                    await update_workflow_progress(thread_id, output)

                    # Check if workflow is interrupted (awaiting user input)
                    if output.get("requires_user_input"):
                        workflow_data["requires_input"] = True
                        workflow_data["status"] = WorkflowStatus.AWAITING_INPUT
                        workflow_data["message"] = "Waiting for user input..."
                        logger.info(f"Workflow {thread_id} paused for user input")
                        return

                # Workflow completed successfully
                workflow_data["status"] = WorkflowStatus.COMPLETED
                workflow_data["current_step"] = "completed"
                workflow_data["progress_percentage"] = 100
                workflow_data["message"] = "Analysis completed successfully"
                workflow_data["updated_at"] = datetime.utcnow().isoformat()

                # Store final results - use the last result state which contains everything
                if result:
                    # Try multiple possible keys for each document type
                    final_results = {
                        "requirements_document": (
                            result.get("requirements_document") or
                            result.get("requirements_doc") or
                            result.get("requirements_definition") or
                            "# 요구사항 문서\n\n이 문서는 현재 생성 중입니다."
                        ),
                        "user_journey_map": (
                            result.get("user_journey_map") or
                            "# 사용자 여정 지도\n\n이 문서는 현재 생성 중입니다."
                        ),
                        "implementation_guide": (
                            result.get("implementation_guide") or
                            result.get("implementation_plan") or
                            "# 구현 가이드\n\n이 문서는 현재 생성 중입니다."
                        ),
                        "tech_recommendations": (
                            result.get("tech_recommendations") or
                            result.get("technology_stack") or
                            result.get("tech_stack") or
                            "# 기술 추천서\n\n이 문서는 현재 생성 중입니다."
                        ),
                        "solution_type": result.get("solution_type") or result.get("recommended_solution_type"),
                        "tech_stack": result.get("tech_stack") or result.get("technology_stack") or []
                    }

                    # Always include final results (don't filter None values for documents)
                    accumulated_results.update(final_results)

                workflow_data["results"] = accumulated_results  # Store accumulated results

                logger.info(f"Workflow {thread_id} completed successfully")

            except Exception as stream_e:
                with open("debug_workflow.txt", "a", encoding="utf-8") as f:
                    f.write(f"ASTREAM ERROR: {stream_e}\n")
                    f.flush()
                logger.error(f"Workflow streaming failed: {stream_e}")
                # Continue with accumulated results even if streaming fails

    except Exception as e:
        # Force debug output for any top-level errors
        with open("debug_workflow.txt", "a", encoding="utf-8") as f:
            f.write(f"TOP-LEVEL WORKFLOW ERROR: {e}\n")
            f.write(f"Exception type: {type(e)}\n")
            import traceback
            f.write(f"Traceback: {traceback.format_exc()}\n")
            f.flush()

        logger.error(f"Workflow {thread_id} failed: {e}")

        # CRITICAL: Generate fallback documents if workflow fails
        workflow_data["status"] = WorkflowStatus.COMPLETED  # Mark as completed despite error
        workflow_data["progress_percentage"] = 100
        workflow_data["message"] = "Analysis completed with fallback documents"
        workflow_data["updated_at"] = datetime.utcnow().isoformat()

        # Generate all 4 required documents as fallback
        fallback_results = {
            "requirements_document": generate_fallback_requirements_document(initial_state),
            "user_journey_map": generate_fallback_user_journey_map(initial_state),
            "implementation_guide": generate_fallback_implementation_guide(initial_state),
            "tech_recommendations": generate_fallback_tech_recommendations(initial_state),
        }
        workflow_data["results"] = fallback_results
        logger.info(f"Generated fallback documents for workflow {thread_id}")


async def resume_langgraph_workflow(thread_id: str, updated_state: ProblemSolvingState):
    """
    Background task to resume a paused LangGraph workflow
    
    Args:
        thread_id: Unique workflow thread identifier
        updated_state: Updated state with user input
    """
    workflow_data = active_workflows.get(thread_id)
    if not workflow_data:
        logger.error(f"Workflow data not found for thread {thread_id}")
        return
    
    try:
        # Create workflow graph
        workflow = create_workflow_graph(enable_human_loop=settings.ENABLE_HUMAN_LOOP)
        
        # Configure thread for checkpointing with increased recursion limit
        thread_config = {
            "configurable": {"thread_id": thread_id},
            "recursion_limit": 50
        }
        
        # Update state to show processing
        workflow_data["status"] = WorkflowStatus.PROCESSING
        workflow_data["message"] = "Processing user input..."
        
        # Use async context manager following exact Context7 pattern
        from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
        async with AsyncSqliteSaver.from_conn_string(":memory:") as checkpointer:
            # Compile workflow with checkpointer
            compiled_workflow = workflow.compile(checkpointer=checkpointer)
            # Resume workflow execution from where it left off
            accumulated_results = workflow_data.get("results", {})  # Get existing results
            result = None
            async for output in compiled_workflow.astream(
                None,  # Continue from checkpoint
                config=thread_config,
                stream_mode="values"
            ):
                result = output
                
                # Accumulate results from each step
                if output.get("requirements_document"):
                    accumulated_results["requirements_document"] = output["requirements_document"]
                if output.get("user_journey_map"):
                    accumulated_results["user_journey_map"] = output["user_journey_map"]
                if output.get("implementation_guide"):
                    accumulated_results["implementation_guide"] = output["implementation_guide"]
                if output.get("tech_recommendations"):
                    accumulated_results["tech_recommendations"] = output["tech_recommendations"]
                if output.get("solution_type"):
                    accumulated_results["solution_type"] = output["solution_type"]
                if output.get("technology_stack"):
                    accumulated_results["tech_stack"] = output["technology_stack"]
                
                # Update workflow state and progress
                await update_workflow_progress(thread_id, output)
                
                # Check if workflow is interrupted again
                if output.get("requires_user_input"):
                    workflow_data["requires_input"] = True
                    workflow_data["status"] = WorkflowStatus.AWAITING_INPUT
                    workflow_data["message"] = "Waiting for additional user input..."
                    logger.info(f"Workflow {thread_id} paused again for user input")
                    return
            
            # Workflow completed successfully
            workflow_data["status"] = WorkflowStatus.COMPLETED
            workflow_data["current_step"] = "completed"
            workflow_data["progress_percentage"] = 100
            workflow_data["message"] = "Analysis completed successfully"
            workflow_data["updated_at"] = datetime.utcnow().isoformat()
            workflow_data["results"] = accumulated_results  # Store accumulated results
            
            logger.info(f"Workflow {thread_id} resumed and completed successfully")
        
    except Exception as e:
        logger.error(f"Failed to resume workflow {thread_id}: {e}")
        workflow_data["status"] = WorkflowStatus.ERROR
        workflow_data["message"] = f"Workflow failed: {str(e)}"
        workflow_data["updated_at"] = datetime.utcnow().isoformat()


def generate_fallback_requirements_document(initial_state: ProblemSolvingState) -> str:
    """Generate fallback requirements document"""
    problem_desc = initial_state.get("problem_description", "사용자 문제")
    return f"""# 📋 요구사항 정의서 (SRS)

## 1. 프로젝트 개요
**문제 설명**: {problem_desc}

## 2. 기능적 요구사항
- 자동화된 업무 처리 시스템 구축
- 사용자 친화적 인터페이스 제공
- 실시간 데이터 처리 및 분석

## 3. 비기능적 요구사항
- 처리 속도: 평균 응답시간 2초 이내
- 신뢰성: 99.5% 이상 가용성
- 확장성: 사용자 증가 시 확장 가능

## 4. 시스템 요구사항
- Python 3.8 이상
- 메모리: 최소 4GB RAM
- 저장공간: 최소 1GB 여유 공간

## 5. 제약사항
- 기존 시스템과의 호환성 유지 필요
- 보안 요구사항 준수 필수
- 개발 기간: 4-6주 내 완성

🤖 **AI 생성 문서**: 실제 요구사항에 맞게 수정해주세요.
"""


def generate_fallback_user_journey_map(initial_state: ProblemSolvingState) -> str:
    """Generate fallback user journey map"""
    problem_desc = initial_state.get("problem_description", "사용자 문제")
    return f"""# 🗺️ 사용자 여정 지도

## 현재 상황 (As-Is)
**문제**: {problem_desc}

### 현재 프로세스
1. **수동 작업 단계**
   - ⏱️ 시간 소요: 평균 2-3시간
   - 😰 어려움: 반복적이고 오류 발생 가능

2. **데이터 처리**
   - 📊 수동 데이터 입력
   - 🔍 수작업 검증 필요

3. **결과 정리**
   - 📝 수동 보고서 작성
   - 📧 개별 공유 필요

## 개선된 상황 (To-Be)
### 자동화된 프로세스
1. **자동 데이터 처리**
   - ⚡ 시간 단축: 15-30분
   - ✅ 자동 검증 및 오류 감지

2. **실시간 분석**
   - 📈 즉시 결과 생성
   - 🎯 정확도 향상

3. **자동 보고**
   - 📋 자동 보고서 생성
   - 🔄 실시간 공유 및 업데이트

## 개선 효과
- ⏰ **시간 절약**: 80% 이상 작업시간 단축
- 🎯 **정확도**: 95% 이상 정확도 향상
- 😊 **사용자 만족도**: 업무 부담 크게 감소

🤖 **AI 생성 문서**: 실제 상황에 맞게 구체화해주세요.
"""


def generate_fallback_implementation_guide(initial_state: ProblemSolvingState) -> str:
    """Generate fallback implementation guide"""
    problem_desc = initial_state.get("problem_description", "사용자 문제")
    return f"""# 🚀 구현 가이드

## 프로젝트 개요
**해결할 문제**: {problem_desc}

## 1. 개발 환경 설정
```bash
# Python 가상환경 생성
python -m venv atm_env
source atm_env/bin/activate  # Windows: atm_env\\Scripts\\activate

# 필요한 라이브러리 설치
pip install fastapi uvicorn pandas openpyxl python-multipart
```

## 2. 프로젝트 구조
```
project/
├── main.py              # FastAPI 메인 애플리케이션
├── modules/
│   ├── data_processor.py # 데이터 처리 모듈
│   ├── automation.py    # 자동화 로직
│   └── utils.py         # 유틸리티 함수들
├── templates/           # HTML 템플릿
├── static/             # CSS, JS 파일들
└── requirements.txt    # 의존성 목록
```

## 3. 핵심 구현 코드
### 3.1 메인 애플리케이션 (main.py)
```python
from fastapi import FastAPI, File, UploadFile
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import pandas as pd

app = FastAPI(title="업무 자동화 도구")
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

@app.post("/process")
async def process_data(file: UploadFile = File(...)):
    # 파일 처리 로직
    content = await file.read()
    df = pd.read_excel(content)

    # 데이터 처리
    processed_data = process_business_logic(df)

    return {"status": "success", "data": processed_data}

def process_business_logic(df):
    # 비즈니스 로직 구현
    # TODO: 실제 업무에 맞게 수정
    return df.to_dict('records')
```

## 4. 실행 방법
```bash
# 개발 서버 실행
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# 브라우저에서 접속
# http://localhost:8000
```

## 5. 다음 단계
1. **테스트 데이터로 검증**
2. **사용자 피드백 수집**
3. **기능 개선 및 확장**
4. **프로덕션 배포 준비**

🤖 **AI 생성 문서**: 실제 비즈니스 로직에 맞게 수정해주세요.
"""


def generate_fallback_tech_recommendations(initial_state: ProblemSolvingState) -> str:
    """Generate fallback tech recommendations"""
    return f"""# 💻 기술 스택 추천서

## 추천 기술 스택

### 🐍 백엔드 프레임워크
**FastAPI** (⭐⭐⭐⭐⭐)
- 빠른 개발과 자동 API 문서화
- 타입 힌팅 지원으로 안정성 향상
- 비동기 처리로 고성능 보장

### 📊 데이터 처리
**Pandas** (⭐⭐⭐⭐⭐)
- Excel, CSV 파일 처리 최적화
- 강력한 데이터 변환 기능
- 광범위한 커뮤니티 지원

### 🎨 프론트엔드 (선택사항)
**Vanilla JavaScript + Bootstrap**
- 간단하고 빠른 UI 구현
- 학습 곡선이 낮음
- 유지보수 용이

### 🗄️ 데이터베이스 (확장 시)
**SQLite** → **PostgreSQL**
- 초기: SQLite로 빠른 프로토타입
- 확장: PostgreSQL로 확장성 확보

## 개발 도구 추천

### 📝 IDE/에디터
- **VS Code** + Python 확장
- **PyCharm Community Edition**

### 🔧 유틸리티
```bash
# 필수 라이브러리
pip install fastapi uvicorn
pip install pandas openpyxl xlsxwriter
pip install python-multipart jinja2
pip install python-dotenv  # 환경변수 관리
```

### 📦 패키지 관리
```bash
# 의존성 관리
pip freeze > requirements.txt

# 가상환경 생성 (권장)
python -m venv venv
```

## 배포 옵션

### 🚀 간단한 배포
1. **로컬 네트워크 배포**
   ```bash
   uvicorn main:app --host 0.0.0.0 --port 8000
   ```

2. **클라우드 배포 (추후)**
   - **Heroku**: 무료 시작, 쉬운 배포
   - **AWS EC2**: 더 많은 제어권
   - **Google Cloud Run**: 서버리스 옵션

## 학습 리소스 📚
- **FastAPI 공식 문서**: https://fastapi.tiangolo.com/ko/
- **Pandas 튜토리얼**: https://pandas.pydata.org/docs/
- **Python 자동화**: "파이썬으로 지루한 일 자동화하기" 책

## 예상 개발 일정 📅
- **1-2주**: 기본 기능 구현
- **3-4주**: 테스트 및 개선
- **5-6주**: 최종 배포 및 문서화

🤖 **AI 생성 문서**: 실제 요구사항에 맞게 조정해주세요.
"""


async def update_workflow_progress(thread_id: str, state_output: Dict[str, Any]):
    """
    Update workflow progress based on state output
    
    Args:
        thread_id: Workflow thread identifier
        state_output: Current state output from LangGraph
    """
    workflow_data = active_workflows.get(thread_id)
    if not workflow_data:
        return
    
    # Map workflow steps to progress percentages
    step_progress = {
        "analyze_problem": 20,
        "collect_context": 40,
        "generate_requirements": 60,
        "design_solution": 80,
        "create_guide": 90
    }
    
    current_step = state_output.get("current_step", "unknown")
    progress = step_progress.get(current_step, workflow_data["progress_percentage"])
    
    # Update workflow metadata
    workflow_data["current_step"] = current_step
    workflow_data["progress_percentage"] = progress
    workflow_data["updated_at"] = datetime.utcnow().isoformat()
    workflow_data["state"] = state_output
    
    # Update status based on current step
    if current_step == "analyze_problem":
        workflow_data["status"] = WorkflowStatus.ANALYZING
        workflow_data["message"] = "Analyzing problem structure..."
    elif current_step == "collect_context":
        workflow_data["status"] = WorkflowStatus.COLLECTING_CONTEXT
        workflow_data["message"] = "Collecting additional context..."
    elif current_step == "generate_requirements":
        workflow_data["status"] = WorkflowStatus.GENERATING_REQUIREMENTS
        workflow_data["message"] = "Generating requirements document..."
    elif current_step == "design_solution":
        workflow_data["status"] = WorkflowStatus.DESIGNING_SOLUTION
        workflow_data["message"] = "Designing solution architecture..."
    elif current_step == "create_guide":
        workflow_data["status"] = WorkflowStatus.CREATING_GUIDE
        workflow_data["message"] = "Creating implementation guide..."


# Create router for API endpoints
router = APIRouter()

# FastAPI endpoints
@router.get("/api/analysis/status/{thread_id}")
async def get_analysis_status(thread_id: str):
    """Get the current status and results of a workflow analysis"""
    workflow_data = active_workflows.get(thread_id)
    if not workflow_data:
        from fastapi import HTTPException
        raise HTTPException(status_code=404, detail="Workflow not found")
    
    return {
        "thread_id": thread_id,
        "status": workflow_data.get("status"),
        "progress_percentage": workflow_data.get("progress_percentage", 0),
        "current_step": workflow_data.get("current_step"),
        "message": workflow_data.get("message"),
        "results": workflow_data.get("results", {}),
        "updated_at": workflow_data.get("updated_at"),
        "created_at": workflow_data.get("created_at")
    }


@analysis_router.post("/start", response_model=StatusResponse)
async def start_problem_solving(request: ProblemSolvingRequest):
    """Start a new problem analysis workflow"""
    thread_id = str(uuid.uuid4())

    # Create initial state from request
    initial_state = {
        "problem_description": request.problem_description,
        "context_data": request.context_data or {},
        "conversation_history": [],
        "current_step": "analyze_problem",
        "current_status": "analyzing",
        "thread_id": thread_id
    }

    # Store initial workflow data
    workflow_data = {
        "thread_id": thread_id,
        "status": WorkflowStatus.STARTING,
        "progress_percentage": 0,
        "current_step": "initializing",
        "message": "Starting analysis...",
        "results": {},
        "created_at": datetime.utcnow().isoformat(),
        "updated_at": datetime.utcnow().isoformat(),
        "request_data": request.dict(),
        "state": initial_state
    }
    active_workflows[thread_id] = workflow_data

    try:
        # Execute workflow directly with await to ensure all documents are generated
        await run_langgraph_workflow(thread_id, initial_state)

        logger.info(f"Started problem solving workflow {thread_id}")

        return StatusResponse(
            thread_id=thread_id,
            status=WorkflowStatus.PROCESSING,
            progress_percentage=10,
            message="Problem analysis started successfully",
            requires_input=False
        )

    except Exception as e:
        logger.error(f"Failed to start workflow {thread_id}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to start problem solving workflow: {str(e)}"
        )
