name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: "3.9"
  NODE_VERSION: "18"

jobs:
  # Backend testing jobs
  backend-unit-tests:
    name: Backend Unit Tests
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r backend/requirements.txt
        pip install pytest-cov pytest-timeout pytest-xdist
    
    - name: Run backend unit tests
      run: |
        cd backend
        python -m pytest tests/backend/unit \
          --cov=app \
          --cov-report=xml \
          --cov-report=term-missing \
          --junit-xml=test-results.xml \
          --timeout=300 \
          -v
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./backend/coverage.xml
        flags: backend-unit
        name: backend-unit-coverage
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: backend-unit-test-results
        path: backend/test-results.xml

  backend-integration-tests:
    name: Backend Integration Tests
    runs-on: ubuntu-latest
    needs: backend-unit-tests
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r backend/requirements.txt
        pip install pytest-cov pytest-timeout pytest-asyncio
    
    - name: Run integration tests
      run: |
        cd backend
        python -m pytest tests/backend/integration \
          --junit-xml=integration-results.xml \
          --timeout=600 \
          -v
      env:
        REDIS_URL: redis://localhost:6379
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: backend-integration-test-results
        path: backend/integration-results.xml

  # Frontend testing jobs
  frontend-tests:
    name: Frontend Tests
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Create package.json if not exists
      run: |
        cd frontend
        if [ ! -f package.json ]; then
          cat > package.json << 'EOF'
        {
          "name": "atm-frontend",
          "version": "1.0.0",
          "scripts": {
            "test": "jest",
            "test:coverage": "jest --coverage"
          },
          "devDependencies": {
            "jest": "^29.0.0",
            "jest-environment-jsdom": "^29.0.0",
            "jest-junit": "^15.0.0",
            "@babel/core": "^7.0.0",
            "@babel/preset-env": "^7.0.0"
          },
          "jest": {
            "testEnvironment": "jsdom",
            "testMatch": ["**/tests/**/*.js"],
            "collectCoverageFrom": ["js/**/*.js"],
            "coverageReporters": ["text", "html", "json", "lcov"],
            "reporters": ["default", "jest-junit"]
          }
        }
        EOF
        fi
    
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm install
    
    - name: Run frontend tests
      run: |
        cd frontend
        npm run test:coverage
      env:
        JEST_JUNIT_OUTPUT_DIR: ./test-results
        JEST_JUNIT_OUTPUT_NAME: frontend-results.xml
    
    - name: Upload frontend coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./frontend/coverage/lcov.info
        flags: frontend
        name: frontend-coverage
    
    - name: Upload frontend test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: frontend-test-results
        path: frontend/test-results/

  # End-to-end testing
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: [backend-integration-tests, frontend-tests]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install backend dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r backend/requirements.txt
    
    - name: Start backend server
      run: |
        cd backend
        python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        sleep 10  # Wait for server to start
      env:
        ENVIRONMENT: test
    
    - name: Wait for backend to be ready
      run: |
        timeout 30 bash -c 'until curl -f http://localhost:8000/api/health; do sleep 2; done'
    
    - name: Run system tests
      run: |
        python -m pytest tests/system \
          --junit-xml=e2e-results.xml \
          --timeout=900 \
          -v
      env:
        API_BASE_URL: http://localhost:8000
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: e2e-results.xml

  # Performance testing (only on main branch)
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [backend-integration-tests]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r backend/requirements.txt
        pip install locust
    
    - name: Start backend server
      run: |
        cd backend
        python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        sleep 10
      env:
        ENVIRONMENT: test
    
    - name: Run performance tests
      run: |
        python -m pytest tests/backend/performance \
          --junit-xml=perf-results.xml \
          --timeout=1800 \
          -v
      env:
        API_BASE_URL: http://localhost:8000
    
    - name: Upload performance test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: perf-results.xml

  # Quality validation
  quality-tests:
    name: Quality Validation
    runs-on: ubuntu-latest
    needs: [e2e-tests]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r backend/requirements.txt
    
    - name: Run quality validation tests
      run: |
        python -m pytest tests/quality \
          --junit-xml=quality-results.xml \
          --timeout=300 \
          -v
    
    - name: Upload quality test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: quality-test-results
        path: quality-results.xml

  # Code quality checks
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install code quality tools
      run: |
        python -m pip install --upgrade pip
        pip install black isort flake8 mypy pylint bandit safety
    
    - name: Check code formatting with Black
      run: |
        black --check --diff backend/
    
    - name: Check import sorting with isort
      run: |
        isort --check-only --diff backend/
    
    - name: Lint with flake8
      run: |
        flake8 backend/ --max-line-length=88 --extend-ignore=E203,W503
    
    - name: Type checking with mypy
      run: |
        mypy backend/app/ --ignore-missing-imports
      continue-on-error: true  # mypy can be strict
    
    - name: Security scan with bandit
      run: |
        bandit -r backend/app/ -f json -o bandit-report.json
      continue-on-error: true
    
    - name: Check dependencies for security vulnerabilities
      run: |
        safety check --json --output safety-report.json
      continue-on-error: true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Test summary and reporting
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [backend-unit-tests, backend-integration-tests, frontend-tests, e2e-tests, quality-tests]
    if: always()
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Download all test results
      uses: actions/download-artifact@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        pip install junitparser
    
    - name: Generate test summary
      run: |
        python - << 'EOF'
        import os
        import glob
        from junitparser import JUnitXml
        
        # Find all test result files
        result_files = []
        for root, dirs, files in os.walk('.'):
            for file in files:
                if file.endswith('-results.xml') or file == 'test-results.xml':
                    result_files.append(os.path.join(root, file))
        
        total_tests = 0
        total_failures = 0
        total_errors = 0
        total_skipped = 0
        
        print("## Test Summary")
        print()
        
        for result_file in result_files:
            try:
                xml = JUnitXml.fromfile(result_file)
                tests = xml.tests
                failures = xml.failures
                errors = xml.errors
                skipped = xml.skipped
                
                total_tests += tests
                total_failures += failures
                total_errors += errors
                total_skipped += skipped
                
                suite_name = os.path.basename(os.path.dirname(result_file))
                print(f"- **{suite_name}**: {tests} tests, {failures} failures, {errors} errors, {skipped} skipped")
            except Exception as e:
                print(f"- **{result_file}**: Error parsing results - {e}")
        
        print()
        print(f"**Total**: {total_tests} tests, {total_failures} failures, {total_errors} errors, {total_skipped} skipped")
        
        success_rate = ((total_tests - total_failures - total_errors) / total_tests * 100) if total_tests > 0 else 0
        print(f"**Success Rate**: {success_rate:.1f}%")
        
        # Set exit code for workflow
        if total_failures > 0 or total_errors > 0:
            print("❌ Tests failed")
            exit(1)
        else:
            print("✅ All tests passed")
            exit(0)
        EOF

  # Deployment (only on main branch after all tests pass)
  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    needs: [test-summary, performance-tests, code-quality]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy application
      run: |
        echo "🚀 Deploying application..."
        # Add your deployment logic here
        # For example:
        # - Build Docker images
        # - Deploy to staging/production
        # - Update infrastructure
        echo "✅ Deployment completed"
    
    - name: Notify deployment
      run: |
        echo "📢 Deployment notification sent"
        # Add notification logic here (Slack, email, etc.)

# Workflow-level settings
env:
  # Ensure consistent behavior across all jobs
  PYTHONUNBUFFERED: "1"
  PYTEST_CURRENT_TEST: "1"
  
# Security settings
permissions:
  contents: read
  actions: read
  checks: write
  pull-requests: write